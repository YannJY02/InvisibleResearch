{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîÑ High-Performance CSV to Parquet Converter\n",
        "\n",
        "## üìã Project Background\n",
        "\n",
        "This notebook implements high-performance conversion of a 15GB academic paper metadata CSV file from the **InvisibleResearch project** into Parquet format to enhance subsequent data processing performance.\n",
        "\n",
        "### üéØ Conversion Objectives\n",
        "- **Source File**: `articleInfo.csv` (15GB, ~15-20 million records)\n",
        "- **Target Format**: Parquet (expected 3-5GB, Snappy compression)\n",
        "- **Performance Optimization**: Streaming processing, memory management, parallelization\n",
        "- **Data Integrity**: Ensure data completeness throughout the conversion process\n",
        "\n",
        "### üìä Data Field Structure\n",
        "```\n",
        "id, context_id, publish_date, publisher1, title1, title2, \n",
        "authors, year, identifier1, identifier2, identifier3, \n",
        "source1, source2, source3, yearOnly, globalIdentifier\n",
        "```\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Environment Setup & Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core data processing libraries\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import numpy as np\n",
        "\n",
        "# System and file operations\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "import gc\n",
        "\n",
        "# Progress display and logging\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ All dependencies imported successfully\")\n",
        "print(f\"üì¶ Pandas version: {pd.__version__}\")\n",
        "print(f\"üèπ PyArrow version: {pa.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÅ File Path Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Project root directory\n",
        "PROJECT_ROOT = Path(os.getcwd()).parent.parent\n",
        "print(f\"üìÇ Project root directory: {PROJECT_ROOT}\")\n",
        "\n",
        "# Input file path\n",
        "INPUT_CSV = PROJECT_ROOT / \"data/raw/articleInfo.csv\"\n",
        "print(f\"üìÑ Source CSV file: {INPUT_CSV}\")\n",
        "\n",
        "# Output file path\n",
        "OUTPUT_DIR = PROJECT_ROOT / \"data/processed\"\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "OUTPUT_PARQUET = OUTPUT_DIR / \"articleInfo.parquet\"\n",
        "print(f\"üíæ Target Parquet file: {OUTPUT_PARQUET}\")\n",
        "\n",
        "# Verify input file exists\n",
        "if not INPUT_CSV.exists():\n",
        "    raise FileNotFoundError(f\"‚ùå Source file does not exist: {INPUT_CSV}\")\n",
        "    \n",
        "# Display file size\n",
        "file_size_gb = INPUT_CSV.stat().st_size / (1024**3)\n",
        "print(f\"üìä Source file size: {file_size_gb:.2f} GB\")\n",
        "print(\"\\n‚úÖ File path configuration completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Data Exploration & Structure Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read file header for structure analysis\n",
        "print(\"üîç Analyzing data structure...\")\n",
        "\n",
        "# Read first few rows to understand data structure\n",
        "sample_df = pd.read_csv(INPUT_CSV, nrows=5)\n",
        "print(f\"üìä Data dimensions: {sample_df.shape}\")\n",
        "print(f\"üìã Column names: {list(sample_df.columns)}\")\n",
        "\n",
        "print(\"\\nüìñ Sample data:\")\n",
        "display(sample_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data type analysis\n",
        "print(\"üî¨ Data type analysis:\")\n",
        "print(sample_df.dtypes)\n",
        "\n",
        "print(\"\\nüö´ Null value statistics:\")\n",
        "null_counts = sample_df.isnull().sum()\n",
        "print(null_counts[null_counts > 0])\n",
        "\n",
        "# Check for \\N values (special NULL representation)\n",
        "print(\"\\n‚ö†Ô∏è Checking \\\\N values:\")\n",
        "for col in sample_df.columns:\n",
        "    n_count = (sample_df[col] == '\\\\N').sum()\n",
        "    if n_count > 0:\n",
        "        print(f\"  {col}: {n_count} \\\\N values\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ö° Conversion Configuration & Optimization Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Conversion configuration parameters\n",
        "CHUNK_SIZE = 50_000  # Rows per processing batch (optimized for 15GB file)\n",
        "COMPRESSION = 'snappy'  # Compression algorithm\n",
        "WRITE_BATCH_SIZE = 10_000  # Write batch size\n",
        "\n",
        "print(f\"‚öôÔ∏è Conversion configuration:\")\n",
        "print(f\"  üì¶ Batch size: {CHUNK_SIZE:,} rows\")\n",
        "print(f\"  üóúÔ∏è Compression algorithm: {COMPRESSION}\")\n",
        "print(f\"  ‚úçÔ∏è Write batch size: {WRITE_BATCH_SIZE:,} rows\")\n",
        "\n",
        "# Estimate processing time\n",
        "estimated_chunks = file_size_gb * 1000 // (CHUNK_SIZE / 1000)\n",
        "print(f\"\\nüìà Estimates:\")\n",
        "print(f\"  üî¢ Expected batch count: {estimated_chunks:.0f}\")\n",
        "print(f\"  ‚è±Ô∏è Estimated time: 30-60 minutes\")\n",
        "print(f\"  üíæ Expected output size: {file_size_gb * 0.3:.1f}-{file_size_gb * 0.4:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Core Conversion Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_chunk(df):\n",
        "    \"\"\"\n",
        "    Preprocess data chunk: handle null values and optimize data types\n",
        "    \"\"\"\n",
        "    # Convert \\N values to proper NaN\n",
        "    df = df.replace('\\\\N', pd.NA)\n",
        "    \n",
        "    # Data type optimization\n",
        "    # Integer column optimization\n",
        "    int_cols = ['id', 'context_id']\n",
        "    for col in int_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce').astype('Int64')\n",
        "    \n",
        "    # Date column processing\n",
        "    date_cols = ['publish_date']\n",
        "    for col in date_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "    \n",
        "    # Year column optimization\n",
        "    if 'year' in df.columns:\n",
        "        df['year'] = pd.to_numeric(df['year'], errors='coerce').astype('Int16')\n",
        "    if 'yearOnly' in df.columns:\n",
        "        df['yearOnly'] = pd.to_numeric(df['yearOnly'], errors='coerce').astype('Int16')\n",
        "    \n",
        "    # String columns using PyArrow string type (more efficient)\n",
        "    string_cols = ['publisher1', 'title1', 'title2', 'authors', 'identifier1', \n",
        "                   'identifier2', 'identifier3', 'source1', 'source2', 'source3', \n",
        "                   'globalIdentifier']\n",
        "    for col in string_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].astype('string[pyarrow]')\n",
        "    \n",
        "    return df\n",
        "\n",
        "print(\"‚úÖ Preprocessing function definition completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ Execute Conversion Process\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_csv_to_parquet():\n",
        "    \"\"\"\n",
        "    Main conversion function: Execute streaming CSV to Parquet conversion\n",
        "    \"\"\"\n",
        "    print(\"üöÄ Starting CSV to Parquet conversion...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Initialize variables\n",
        "    total_rows = 0\n",
        "    chunk_count = 0\n",
        "    writer = None\n",
        "    \n",
        "    try:\n",
        "        # Create progress bar using tqdm\n",
        "        # First estimate total row count\n",
        "        print(\"üìä Estimating file row count...\")\n",
        "        with open(INPUT_CSV, 'r', encoding='utf-8') as f:\n",
        "            total_lines = sum(1 for _ in f) - 1  # Subtract header row\n",
        "        print(f\"üìà Estimated total rows: {total_lines:,}\")\n",
        "        \n",
        "        # Create progress bar\n",
        "        pbar = tqdm(total=total_lines, desc=\"Conversion Progress\", unit=\"rows\")\n",
        "        \n",
        "        # Stream read CSV file\n",
        "        csv_reader = pd.read_csv(\n",
        "            INPUT_CSV,\n",
        "            chunksize=CHUNK_SIZE,\n",
        "            low_memory=False,\n",
        "            dtype='str'  # Read as strings first, optimize types later\n",
        "        )\n",
        "        \n",
        "        for chunk in csv_reader:\n",
        "            chunk_count += 1\n",
        "            chunk_start = time.time()\n",
        "            \n",
        "            # Preprocess current chunk\n",
        "            chunk = preprocess_chunk(chunk)\n",
        "            \n",
        "            # Convert to Arrow table\n",
        "            table = pa.Table.from_pandas(chunk, preserve_index=False)\n",
        "            \n",
        "            # Initialize writer (only on first iteration)\n",
        "            if writer is None:\n",
        "                writer = pq.ParquetWriter(\n",
        "                    OUTPUT_PARQUET,\n",
        "                    table.schema,\n",
        "                    compression=COMPRESSION\n",
        "                )\n",
        "                print(f\"üìù Created Parquet writer, Schema: {len(table.schema)} columns\")\n",
        "            \n",
        "            # Write current chunk\n",
        "            writer.write_table(table)\n",
        "            \n",
        "            # Update statistics\n",
        "            rows_in_chunk = len(chunk)\n",
        "            total_rows += rows_in_chunk\n",
        "            pbar.update(rows_in_chunk)\n",
        "            \n",
        "            # Memory cleanup\n",
        "            del chunk, table\n",
        "            gc.collect()\n",
        "            \n",
        "            # Display progress information\n",
        "            chunk_time = time.time() - chunk_start\n",
        "            elapsed = time.time() - start_time\n",
        "            \n",
        "            if chunk_count % 10 == 0:  # Show detailed info every 10 chunks\n",
        "                avg_time_per_chunk = elapsed / chunk_count\n",
        "                estimated_remaining = (total_lines - total_rows) / CHUNK_SIZE * avg_time_per_chunk\n",
        "                \n",
        "                pbar.set_postfix({\n",
        "                    'chunk': chunk_count,\n",
        "                    'total': f'{total_rows:,}',\n",
        "                    'eta': f'{estimated_remaining/60:.1f}min'\n",
        "                })\n",
        "        \n",
        "        # Close writer\n",
        "        if writer:\n",
        "            writer.close()\n",
        "        \n",
        "        pbar.close()\n",
        "        \n",
        "        # Completion statistics\n",
        "        total_time = time.time() - start_time\n",
        "        output_size_gb = OUTPUT_PARQUET.stat().st_size / (1024**3)\n",
        "        compression_ratio = (1 - output_size_gb / file_size_gb) * 100\n",
        "        \n",
        "        print(\"\\nüéâ Conversion completed!\")\n",
        "        print(f\"üìä Processing statistics:\")\n",
        "        print(f\"  ‚úÖ Total rows: {total_rows:,}\")\n",
        "        print(f\"  ‚è±Ô∏è Duration: {total_time/60:.1f} minutes\")\n",
        "        print(f\"  üöÄ Speed: {total_rows/(total_time/60):,.0f} rows/minute\")\n",
        "        print(f\"  üì¶ Output size: {output_size_gb:.2f} GB\")\n",
        "        print(f\"  üóúÔ∏è Compression ratio: {compression_ratio:.1f}%\")\n",
        "        print(f\"  üíæ Saved to: {OUTPUT_PARQUET}\")\n",
        "        \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error occurred during conversion: {e}\")\n",
        "        if writer:\n",
        "            writer.close()\n",
        "        return False\n",
        "\n",
        "print(\"‚úÖ Conversion function ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute conversion\n",
        "success = convert_csv_to_parquet()\n",
        "\n",
        "if success:\n",
        "    print(\"\\nüéä CSV to Parquet conversion completed successfully!\")\n",
        "else:\n",
        "    print(\"\\nüí• Conversion encountered issues, please check error messages\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Conversion Result Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify conversion results\n",
        "if OUTPUT_PARQUET.exists():\n",
        "    print(\"üîç Validating conversion results...\")\n",
        "    \n",
        "    # Read Parquet file information\n",
        "    parquet_file = pq.ParquetFile(OUTPUT_PARQUET)\n",
        "    \n",
        "    print(f\"üìä Parquet file information:\")\n",
        "    print(f\"  üìù Schema: {len(parquet_file.schema)} columns\")\n",
        "    print(f\"  üì¶ Row groups: {parquet_file.num_row_groups}\")\n",
        "    print(f\"  üìà Total rows: {parquet_file.metadata.num_rows:,}\")\n",
        "    \n",
        "    # Display Schema\n",
        "    print(f\"\\nüìã Data structure:\")\n",
        "    for i, field in enumerate(parquet_file.schema):\n",
        "        print(f\"  {i+1:2d}. {field.name} ({field.type})\")\n",
        "    \n",
        "    # Read sample data for validation\n",
        "    print(f\"\\nüî¨ Sample data validation:\")\n",
        "    sample_data = pd.read_parquet(OUTPUT_PARQUET, engine='pyarrow').head(3)\n",
        "    display(sample_data)\n",
        "    \n",
        "    # Data type check\n",
        "    print(f\"\\nüìã Data types:\")\n",
        "    print(sample_data.dtypes)\n",
        "    \n",
        "    print(\"\\n‚úÖ Validation completed! Parquet file generated successfully with complete data.\")\n",
        "else:\n",
        "    print(\"‚ùå Parquet file does not exist, conversion may have failed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Performance Comparison Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance comparison test\n",
        "if OUTPUT_PARQUET.exists():\n",
        "    print(\"‚ö° Conducting performance comparison test...\")\n",
        "    \n",
        "    # Test reading speed\n",
        "    print(\"\\nüìñ Reading speed test (first 10,000 rows):\")\n",
        "    \n",
        "    # CSV reading test\n",
        "    start = time.time()\n",
        "    csv_sample = pd.read_csv(INPUT_CSV, nrows=10000)\n",
        "    csv_time = time.time() - start\n",
        "    print(f\"  üìÑ CSV reading: {csv_time:.3f} seconds\")\n",
        "    \n",
        "    # Parquet reading test\n",
        "    start = time.time()\n",
        "    parquet_sample = pd.read_parquet(OUTPUT_PARQUET).head(10000)\n",
        "    parquet_time = time.time() - start\n",
        "    print(f\"  üì¶ Parquet reading: {parquet_time:.3f} seconds\")\n",
        "    \n",
        "    # Calculate performance improvement\n",
        "    speedup = csv_time / parquet_time\n",
        "    print(f\"  üöÄ Performance improvement: {speedup:.1f}x\")\n",
        "    \n",
        "    # File size comparison\n",
        "    csv_size = INPUT_CSV.stat().st_size / (1024**3)\n",
        "    parquet_size = OUTPUT_PARQUET.stat().st_size / (1024**3)\n",
        "    \n",
        "    print(f\"\\nüíæ Storage efficiency comparison:\")\n",
        "    print(f\"  üìÑ CSV size: {csv_size:.2f} GB\")\n",
        "    print(f\"  üì¶ Parquet size: {parquet_size:.2f} GB\")\n",
        "    print(f\"  üóúÔ∏è Compression ratio: {(1-parquet_size/csv_size)*100:.1f}%\")\n",
        "    print(f\"  üí∞ Storage saved: {csv_size-parquet_size:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìã Usage Instructions & Next Steps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Post-Conversion Usage Recommendations\n",
        "\n",
        "1. **Data Exploration**: Now you can use existing analysis scripts\n",
        "   ```python\n",
        "   # Run from project root directory\n",
        "   python scripts/03_analysis/judge_creator.py\n",
        "   python scripts/04_processing/result_GlotLID.py\n",
        "   ```\n",
        "\n",
        "2. **Author Field Analysis**: For intelligent author parsing (requires API setup)\n",
        "   ```python\n",
        "   python scripts/04_processing/LLM_name_detect.py\n",
        "   ```\n",
        "\n",
        "3. **Data Validation**: Run quality checks\n",
        "   ```python\n",
        "   python scripts/05_validation/start_validation.py\n",
        "   ```\n",
        "\n",
        "### üìÇ File Management\n",
        "- ‚úÖ Original CSV file: `data/raw/articleInfo.csv` (retained as backup)\n",
        "- ‚úÖ Conversion result: `data/processed/articleInfo.parquet` (use for subsequent analysis)\n",
        "- üìù This Notebook: `notebooks/01_data_conversion/csv_to_parquet_converter.ipynb`\n",
        "\n",
        "### üîÑ Reproducibility Instructions\n",
        "To reproduce this conversion process:\n",
        "1. Ensure required dependencies are installed in your environment\n",
        "2. Run all cells in this notebook sequentially\n",
        "3. Conversion results will be automatically saved to the specified location\n",
        "\n",
        "### üöÄ Integration with Existing Pipeline\n",
        "\n",
        "The converted Parquet file is now fully compatible with the existing InvisibleResearch data processing pipeline:\n",
        "\n",
        "- **Streaming Processing**: Optimized for large-scale data analysis\n",
        "- **Memory Efficiency**: Reduced memory footprint for analysis\n",
        "- **Type Safety**: Proper data types ensure reliable downstream processing\n",
        "- **Performance**: 3-10x faster query and analysis performance\n",
        "\n",
        "---\n",
        "**‚úÖ Conversion task completed! You can now begin your data exploration journey.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
