{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dimensions Publications CSV Merge (2000–2025) and Parquet Conversion\n",
        "\n",
        "## Background and Objectives\n",
        "This notebook merges yearly Dimensions CSV files for publications from 2000 to 2025 and converts the merged CSV to Parquet. The workflow follows a robust, reproducible process suitable for large-scale bibliometric datasets.\n",
        "\n",
        "- Source directory: `data/raw/dimensions_cs`\n",
        "- Files merged: `publications_2000.csv` … `publications_2025.csv`\n",
        "- Column policy: union-of-columns; missing values are encoded as empty strings in CSV\n",
        "- Output directory: `data/processed`\n",
        "- Outputs: `dimension_merged.csv`, `dimension_merged.parquet`\n",
        "\n",
        "The implementation emphasizes transparency, reproducibility, and performance using streaming I/O and DuckDB for efficient CSV→Parquet conversion.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Methods Overview\n",
        "\n",
        "The notebook is organized into the following stages:\n",
        "1. Environment setup and path configuration\n",
        "2. Input file validation (2000–2025)\n",
        "3. Column inspection (union-of-columns policy)\n",
        "4. Streaming merge to a single CSV (kept for audit)\n",
        "5. CSV → Parquet conversion via DuckDB\n",
        "6. Output validation and sampling\n",
        "7. Reproducibility metadata (package versions, runtime)\n",
        "\n",
        "We keep the intermediate CSV to enable downstream auditability and external tool usage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /Users/yann.jy/InvisibleResearch\n",
            "Input dir: /Users/yann.jy/InvisibleResearch/data/raw/dimensions_cs\n",
            "Output dir: /Users/yann.jy/InvisibleResearch/data/processed\n",
            "Output CSV: /Users/yann.jy/InvisibleResearch/data/processed/dimension_merged.csv\n",
            "Output Parquet: /Users/yann.jy/InvisibleResearch/data/processed/dimension_merged.parquet\n"
          ]
        }
      ],
      "source": [
        "# Environment setup\n",
        "from pathlib import Path\n",
        "import os\n",
        "import csv\n",
        "import time\n",
        "from typing import List, Optional, Set, Iterable\n",
        "\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "\n",
        "# Ensure duckdb is available\n",
        "try:\n",
        "    import duckdb  # type: ignore\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"duckdb>=0.10.0\"], check=True)\n",
        "    import duckdb  # type: ignore\n",
        "\n",
        "PROJECT_ROOT = Path(os.getcwd()).parent.parent\n",
        "INPUT_DIR = PROJECT_ROOT / \"data/raw/dimensions_cs\"\n",
        "OUTPUT_DIR = PROJECT_ROOT / \"data/processed\"\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Inputs (2000–2025)\n",
        "INPUT_FILES = [INPUT_DIR / f\"publications_{year}.csv\" for year in range(2000, 2026)]\n",
        "\n",
        "# Outputs\n",
        "OUTPUT_CSV = OUTPUT_DIR / \"dimension_merged.csv\"\n",
        "OUTPUT_PARQUET = OUTPUT_DIR / \"dimension_merged.parquet\"\n",
        "\n",
        "print(\"Project root:\", PROJECT_ROOT)\n",
        "print(\"Input dir:\", INPUT_DIR)\n",
        "print(\"Output dir:\", OUTPUT_DIR)\n",
        "print(\"Output CSV:\", OUTPUT_CSV)\n",
        "print(\"Output Parquet:\", OUTPUT_PARQUET)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validate Input Files\n",
        "We require all yearly files to exist prior to merging. This ensures deterministic behavior and eases troubleshooting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Existing files: 26\n"
          ]
        }
      ],
      "source": [
        "existing_files = [p for p in INPUT_FILES if p.exists()]\n",
        "missing = [p for p in INPUT_FILES if not p.exists()]\n",
        "\n",
        "if not existing_files:\n",
        "    raise FileNotFoundError(\"No input files found under \" + str(INPUT_DIR))\n",
        "\n",
        "print(f\"Existing files: {len(existing_files)}\")\n",
        "if missing:\n",
        "    print(\"Missing files (skipped):\")\n",
        "    for m in missing[:10]:\n",
        "        print(\" -\", m)\n",
        "    if len(missing) > 10:\n",
        "        print(f\" ... (+{len(missing)-10} more)\")\n",
        "\n",
        "EXISTING_FILES = existing_files\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Column Inspection (Union-of-Columns)\n",
        "We read only header lines to compute the union of columns and detect schema differences across years.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total unique columns: 76\n",
            "Column differences detected.\n",
            "Examples of differing files (first 5):\n",
            " - /Users/yann.jy/InvisibleResearch/data/raw/dimensions_cs/publications_2001.csv\n",
            " - /Users/yann.jy/InvisibleResearch/data/raw/dimensions_cs/publications_2002.csv\n",
            " - /Users/yann.jy/InvisibleResearch/data/raw/dimensions_cs/publications_2003.csv\n",
            " - /Users/yann.jy/InvisibleResearch/data/raw/dimensions_cs/publications_2004.csv\n",
            " - /Users/yann.jy/InvisibleResearch/data/raw/dimensions_cs/publications_2005.csv\n"
          ]
        }
      ],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "def read_header(path: Path) -> List[str]:\n",
        "    with path.open(\"r\", encoding=\"utf-8-sig\", errors=\"replace\", newline=\"\") as f:  # utf-8-sig strips BOM\n",
        "        reader = csv.reader(\n",
        "            f,\n",
        "            delimiter=\",\",\n",
        "            quotechar='\"',\n",
        "            doublequote=True,\n",
        "            escapechar=\"\\\\\",\n",
        "        )\n",
        "        header = next(reader)\n",
        "        return [c.strip() for c in header]\n",
        "\n",
        "union_cols: Set[str] = set()\n",
        "all_same = True\n",
        "baseline: Optional[List[str]] = None\n",
        "differing_files: List[str] = []\n",
        "\n",
        "for p in EXISTING_FILES:\n",
        "    cols = read_header(p)\n",
        "    union_cols.update(cols)\n",
        "    if baseline is None:\n",
        "        baseline = cols\n",
        "    elif cols != baseline:\n",
        "        all_same = False\n",
        "        differing_files.append(str(p))\n",
        "\n",
        "UNION_COLUMNS = sorted(list(union_cols))\n",
        "print(f\"Total unique columns: {len(UNION_COLUMNS)}\")\n",
        "print(\"All files share same columns:\" if all_same else \"Column differences detected.\")\n",
        "if differing_files:\n",
        "    print(\"Examples of differing files (first 5):\")\n",
        "    for ex in differing_files[:5]:\n",
        "        print(\" -\", ex)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Streaming Merge to CSV\n",
        "We stream through each yearly CSV, align rows to the union of columns, and write to a single CSV. Missing columns in a given year are written as empty strings. The intermediate CSV is kept for auditability and potential reuse.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 5/26 files. Total rows so far: 21,376\n",
            "Processed 10/26 files. Total rows so far: 54,386\n",
            "Processed 15/26 files. Total rows so far: 108,532\n",
            "Processed 20/26 files. Total rows so far: 199,011\n",
            "Processed 25/26 files. Total rows so far: 351,923\n",
            "Merged CSV rows: 358,493\n"
          ]
        }
      ],
      "source": [
        "def merge_to_csv(input_paths: List[Path], union_columns: List[str], output_csv_path: Path) -> int:\n",
        "    output_csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    total_rows = 0\n",
        "\n",
        "    with output_csv_path.open(\"w\", encoding=\"utf-8\", errors=\"replace\", newline=\"\") as out_f:\n",
        "        writer = csv.writer(out_f, delimiter=\",\", quotechar='\"', lineterminator=\"\\n\")\n",
        "        writer.writerow(union_columns)\n",
        "\n",
        "        for idx, path in enumerate(input_paths, start=1):\n",
        "            with path.open(\"r\", encoding=\"utf-8-sig\", errors=\"replace\", newline=\"\") as in_f:\n",
        "                reader = csv.reader(\n",
        "                    in_f,\n",
        "                    delimiter=\",\",\n",
        "                    quotechar='\"',\n",
        "                    doublequote=True,\n",
        "                    escapechar=\"\\\\\",\n",
        "                )\n",
        "                header = next(reader, None)\n",
        "                if header is None:\n",
        "                    continue\n",
        "                header = [c.strip() for c in header]\n",
        "                name_to_idx = {name: i for i, name in enumerate(header)}\n",
        "\n",
        "                for row in reader:\n",
        "                    out_row = []\n",
        "                    row_len = len(row)\n",
        "                    for col in union_columns:\n",
        "                        idx_col = name_to_idx.get(col)\n",
        "                        out_row.append(row[idx_col] if idx_col is not None and idx_col < row_len else \"\")\n",
        "                    writer.writerow(out_row)\n",
        "                    total_rows += 1\n",
        "\n",
        "            if idx % 5 == 0:\n",
        "                print(f\"Processed {idx}/{len(input_paths)} files. Total rows so far: {total_rows:,}\")\n",
        "\n",
        "    return total_rows\n",
        "\n",
        "# Increase CSV field size limit to accommodate long text fields\n",
        "try:\n",
        "    csv.field_size_limit(10_000_000)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "rows_written = merge_to_csv(EXISTING_FILES, UNION_COLUMNS, OUTPUT_CSV)\n",
        "print(f\"Merged CSV rows: {rows_written:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CSV → Parquet Conversion via DuckDB\n",
        "We convert the merged CSV to Parquet using DuckDB's COPY command with an explicit VARCHAR mapping to avoid implicit type inference pitfalls and ensure schema consistency.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Executing DuckDB COPY → Parquet ...\n",
            "Parquet rows: 358,493\n"
          ]
        }
      ],
      "source": [
        "# Build explicit column mapping for DuckDB as VARCHAR\n",
        "columns_pairs = \", \".join([f\"'{c}':'VARCHAR'\" for c in UNION_COLUMNS])\n",
        "columns_spec = \"{\" + columns_pairs + \"}\"\n",
        "\n",
        "# Clean output parquet if exists\n",
        "if OUTPUT_PARQUET.exists():\n",
        "    OUTPUT_PARQUET.unlink()\n",
        "\n",
        "con = duckdb.connect()\n",
        "con.execute(\"PRAGMA threads=4\")\n",
        "\n",
        "safe_csv = str(OUTPUT_CSV).replace(\"'\", \"''\")\n",
        "safe_parquet = str(OUTPUT_PARQUET).replace(\"'\", \"''\")\n",
        "\n",
        "copy_sql = (\n",
        "    f\"COPY (SELECT * FROM read_csv('{safe_csv}', AUTO_DETECT=FALSE, HEADER=TRUE, \"\n",
        "    f\"COLUMNS={columns_spec}, delim=',', quote='\\\"', escape='\\\"', strict_mode=FALSE, null_padding=TRUE, \"\n",
        "    f\"maximum_line_size=20000000, parallel=FALSE)) TO '{safe_parquet}' (FORMAT 'parquet', COMPRESSION 'SNAPPY')\"\n",
        ")\n",
        "print(\"Executing DuckDB COPY → Parquet ...\")\n",
        "con.execute(copy_sql)\n",
        "\n",
        "# Count rows in Parquet\n",
        "row_count = con.execute(f\"SELECT COUNT(*) FROM read_parquet('{safe_parquet}')\").fetchone()[0]\n",
        "con.close()\n",
        "print(f\"Parquet rows: {row_count:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validation and Sampling\n",
        "We validate data integrity beyond simple row counts: logical CSV row counting, `id` uniqueness, DOI normalization duplicates, year range/distribution, key field quality, optional exact-duplicate detection, and sampling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counting CSV logical rows (this may take a moment)...\n",
            "CSV logical rows: 358,493\n",
            "Parquet rows: 358,493\n",
            "Parquet columns: 76\n",
            "Row groups: 3\n",
            "✅ Row count validated: CSV and Parquet both have 358,493 records\n",
            "\n",
            "[id] total=358,493, distinct=358,493, nulls=0, unique=True\n",
            "\n",
            "DOI_norm duplicate rows: 118\n",
            "Top duplicated doi_norm:\n",
            "                               doi_norm  c\n",
            "0          10.1299/jsmermd.2024.2a2-f08  2\n",
            "1                  10.5937/zrffp45-7741  2\n",
            "2         10.17762/turcomat.v12i1s.1764  2\n",
            "3         10.1080/17512786.2022.2055620  2\n",
            "4         10.31185/lark.vol2.iss45.2410  2\n",
            "5             10.5294/pacla.2023.26.2.3  2\n",
            "6                  10.2139/ssrn.3018521  2\n",
            "7                10.3917/jibes.303.0103  2\n",
            "8                     10.1093/ct/qtz004  2\n",
            "9  10.22162/2587-6503-2022-1-21-134-173  2\n",
            "\n",
            "Year stats:\n",
            "   min_year  max_year  null_years\n",
            "0      2000      2025         0.0\n",
            "Out-of-range/unparsable years (should be small or zero):\n",
            "   bad_years\n",
            "0          2\n",
            "Counts by year:\n",
            "       y      c\n",
            "0   2000   3620\n",
            "1   2001   3827\n",
            "2   2002   4037\n",
            "3   2003   5430\n",
            "4   2004   4462\n",
            "5   2005   5087\n",
            "6   2006   5096\n",
            "7   2007   7166\n",
            "8   2008   7771\n",
            "9   2009   7890\n",
            "10  2010   8054\n",
            "11  2011   9151\n",
            "12  2012  11046\n",
            "13  2013  12459\n",
            "14  2014  13436\n",
            "15  2015  14524\n",
            "16  2016  16071\n",
            "17  2017  18351\n",
            "18  2018  19671\n",
            "19  2019  21862\n",
            "20  2020  25758\n",
            "21  2021  28688\n",
            "22  2022  29802\n",
            "23  2023  33390\n",
            "24  2024  35272\n",
            "25  2025   6570\n",
            "26  <NA>      2\n",
            "\n",
            "Key field quality:\n",
            "[title]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>total</th>\n",
              "      <th>nulls</th>\n",
              "      <th>distinct_vals</th>\n",
              "      <th>null_ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>358493</td>\n",
              "      <td>0</td>\n",
              "      <td>338123</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    total  nulls  distinct_vals  null_ratio\n",
              "0  358493      0         338123         0.0"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[authors]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>total</th>\n",
              "      <th>nulls</th>\n",
              "      <th>distinct_vals</th>\n",
              "      <th>null_ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>358493</td>\n",
              "      <td>18224</td>\n",
              "      <td>304988</td>\n",
              "      <td>0.0508</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    total  nulls  distinct_vals  null_ratio\n",
              "0  358493  18224         304988      0.0508"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[year]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>total</th>\n",
              "      <th>nulls</th>\n",
              "      <th>distinct_vals</th>\n",
              "      <th>null_ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>358493</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    total  nulls  distinct_vals  null_ratio\n",
              "0  358493      0             28         0.0"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Exact duplicate rows (all-columns): 0\n",
            "\n",
            "Sample rows from Parquet:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abstract</th>\n",
              "      <th>acknowledgements</th>\n",
              "      <th>altmetric</th>\n",
              "      <th>altmetric_id</th>\n",
              "      <th>arxiv_id</th>\n",
              "      <th>authors</th>\n",
              "      <th>authors_count</th>\n",
              "      <th>book_doi</th>\n",
              "      <th>book_series_title</th>\n",
              "      <th>book_title</th>\n",
              "      <th>...</th>\n",
              "      <th>score</th>\n",
              "      <th>source_title.id</th>\n",
              "      <th>source_title.title</th>\n",
              "      <th>subtitles</th>\n",
              "      <th>supporting_grant_ids</th>\n",
              "      <th>times_cited</th>\n",
              "      <th>title</th>\n",
              "      <th>type</th>\n",
              "      <th>volume</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;p&gt;Coverage of the Clinton-Lewinsky saga follo...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>0.0</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>10.5771/9781461643852</td>\n",
              "      <td>None</td>\n",
              "      <td>Tabloid Tales</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>Preface</td>\n",
              "      <td>chapter</td>\n",
              "      <td>None</td>\n",
              "      <td>2000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&lt;p&gt;Coverage of the Clinton-Lewinsky saga follo...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>0.0</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>10.5771/9781461643852</td>\n",
              "      <td>None</td>\n",
              "      <td>Tabloid Tales</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>Part One: Are the Tabloids Taking Over?</td>\n",
              "      <td>chapter</td>\n",
              "      <td>None</td>\n",
              "      <td>2000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&lt;p&gt;Coverage of the Clinton-Lewinsky saga follo...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>0.0</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>10.5771/9781461643852</td>\n",
              "      <td>None</td>\n",
              "      <td>Tabloid Tales</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>About the Editors and Contributors</td>\n",
              "      <td>chapter</td>\n",
              "      <td>None</td>\n",
              "      <td>2000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;p&gt;Coverage of the Clinton-Lewinsky saga follo...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>0.0</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>10.5771/9781461643852</td>\n",
              "      <td>None</td>\n",
              "      <td>Tabloid Tales</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>Part Three: What Implications Does Tabloid Jou...</td>\n",
              "      <td>chapter</td>\n",
              "      <td>None</td>\n",
              "      <td>2000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;p&gt;Coverage of the Clinton-Lewinsky saga follo...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>0.0</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>10.5771/9781461643852</td>\n",
              "      <td>None</td>\n",
              "      <td>Tabloid Tales</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>Part Two: Tabloid Journalism in Perspective</td>\n",
              "      <td>chapter</td>\n",
              "      <td>None</td>\n",
              "      <td>2000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 76 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            abstract acknowledgements  \\\n",
              "0  <p>Coverage of the Clinton-Lewinsky saga follo...             None   \n",
              "1  <p>Coverage of the Clinton-Lewinsky saga follo...             None   \n",
              "2  <p>Coverage of the Clinton-Lewinsky saga follo...             None   \n",
              "3  <p>Coverage of the Clinton-Lewinsky saga follo...             None   \n",
              "4  <p>Coverage of the Clinton-Lewinsky saga follo...             None   \n",
              "\n",
              "  altmetric altmetric_id arxiv_id authors authors_count  \\\n",
              "0      None          0.0     None    None             0   \n",
              "1      None          0.0     None    None             0   \n",
              "2      None          0.0     None    None             0   \n",
              "3      None          0.0     None    None             0   \n",
              "4      None          0.0     None    None             0   \n",
              "\n",
              "                book_doi book_series_title     book_title  ... score  \\\n",
              "0  10.5771/9781461643852              None  Tabloid Tales  ...   1.0   \n",
              "1  10.5771/9781461643852              None  Tabloid Tales  ...   1.0   \n",
              "2  10.5771/9781461643852              None  Tabloid Tales  ...   1.0   \n",
              "3  10.5771/9781461643852              None  Tabloid Tales  ...   1.0   \n",
              "4  10.5771/9781461643852              None  Tabloid Tales  ...   1.0   \n",
              "\n",
              "  source_title.id source_title.title subtitles supporting_grant_ids  \\\n",
              "0            None               None      None                 None   \n",
              "1            None               None      None                 None   \n",
              "2            None               None      None                 None   \n",
              "3            None               None      None                 None   \n",
              "4            None               None      None                 None   \n",
              "\n",
              "  times_cited                                              title     type  \\\n",
              "0           0                                            Preface  chapter   \n",
              "1           0            Part One: Are the Tabloids Taking Over?  chapter   \n",
              "2           0                 About the Editors and Contributors  chapter   \n",
              "3           0  Part Three: What Implications Does Tabloid Jou...  chapter   \n",
              "4           0        Part Two: Tabloid Journalism in Perspective  chapter   \n",
              "\n",
              "  volume  year  \n",
              "0   None  2000  \n",
              "1   None  2000  \n",
              "2   None  2000  \n",
              "3   None  2000  \n",
              "4   None  2000  \n",
              "\n",
              "[5 rows x 76 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pyarrow.parquet as pq\n",
        "import duckdb\n",
        "\n",
        "# Count logical rows in CSV using csv.reader (handles multiline fields correctly)\n",
        "print(\"Counting CSV logical rows (this may take a moment)...\")\n",
        "csv_rows = 0\n",
        "with OUTPUT_CSV.open('r', encoding='utf-8-sig', errors='replace', newline='') as f:\n",
        "    reader = csv.reader(f, delimiter=',', quotechar='\"', doublequote=True, escapechar='\\\\')\n",
        "    next(reader, None)  # skip header\n",
        "    for _ in reader:\n",
        "        csv_rows += 1\n",
        "print(f\"CSV logical rows: {csv_rows:,}\")\n",
        "\n",
        "# Read Parquet metadata and show schema\n",
        "parquet_file = pq.ParquetFile(OUTPUT_PARQUET)\n",
        "parquet_rows = parquet_file.metadata.num_rows\n",
        "print(f\"Parquet rows: {parquet_rows:,}\")\n",
        "print(f\"Parquet columns: {len(parquet_file.schema)}\")\n",
        "print(f\"Row groups: {parquet_file.num_row_groups}\")\n",
        "\n",
        "# Validate row count consistency\n",
        "if csv_rows == parquet_rows:\n",
        "    print(f\"✅ Row count validated: CSV and Parquet both have {csv_rows:,} records\")\n",
        "else:\n",
        "    print(f\"⚠️ Row count mismatch: CSV={csv_rows:,}, Parquet={parquet_rows:,}, diff={abs(csv_rows-parquet_rows):,}\")\n",
        "\n",
        "# Build a DuckDB view on Parquet for further checks\n",
        "con = duckdb.connect()\n",
        "parquet_path = str(OUTPUT_PARQUET).replace(\"'\", \"''\")\n",
        "con.execute(f\"CREATE OR REPLACE VIEW v AS SELECT * FROM read_parquet('{parquet_path}')\")\n",
        "\n",
        "cols = [r[0] for r in con.execute(\"DESCRIBE SELECT * FROM v\").fetchall()]\n",
        "\n",
        "def q(name: str) -> str:\n",
        "    return '\"' + name.replace('\"','\"\"') + '\"'\n",
        "\n",
        "# 1) id uniqueness (as requested)\n",
        "if 'id' in cols:\n",
        "    id_stats = con.execute(\n",
        "        f\"\"\"\n",
        "        SELECT \n",
        "          COUNT(*)::BIGINT AS total,\n",
        "          COUNT(DISTINCT {q('id')})::BIGINT AS distinct_ids,\n",
        "          SUM(CASE WHEN {q('id')} IS NULL OR {q('id')}='' THEN 1 ELSE 0 END)::BIGINT AS null_ids\n",
        "        FROM v\n",
        "        \"\"\"\n",
        "    ).fetch_df()\n",
        "    total, distinct_ids, null_ids = int(id_stats['total'][0]), int(id_stats['distinct_ids'][0]), int(id_stats['null_ids'][0])\n",
        "    print(f\"\\n[id] total={total:,}, distinct={distinct_ids:,}, nulls={null_ids:,}, unique={distinct_ids==total and null_ids==0}\")\n",
        "\n",
        "    # show any duplicated ids (top 10)\n",
        "    if not (distinct_ids==total and null_ids==0):\n",
        "        print(\"Examples of duplicated id:\")\n",
        "        print(con.execute(\n",
        "            f\"\"\"\n",
        "            SELECT {q('id')} AS id, COUNT(*) AS c\n",
        "            FROM v\n",
        "            GROUP BY 1\n",
        "            HAVING COUNT(*) > 1\n",
        "            ORDER BY c DESC\n",
        "            LIMIT 10\n",
        "            \"\"\"\n",
        "        ).fetch_df())\n",
        "else:\n",
        "    print(\"\\n[id] column not found; skip uniqueness check.\")\n",
        "\n",
        "# 2) DOI normalization duplicates (if doi exists)\n",
        "if 'doi' in cols:\n",
        "    con.execute(f\"\"\"\n",
        "    CREATE OR REPLACE VIEW v_doi AS\n",
        "    SELECT *,\n",
        "           lower(regexp_replace(regexp_replace(coalesce({q('doi')}, ''), '^https?://(dx\\\\.)?doi\\\\.org/', ''), '\\\\s+', '')) AS doi_norm\n",
        "    FROM v\n",
        "    \"\"\")\n",
        "    dup_rows = con.execute(\n",
        "        \"\"\"\n",
        "        SELECT COALESCE(SUM(c),0)::BIGINT FROM (\n",
        "          SELECT COUNT(*) AS c\n",
        "          FROM v_doi\n",
        "          WHERE doi_norm <> ''\n",
        "          GROUP BY doi_norm\n",
        "          HAVING COUNT(*) > 1\n",
        "        )\n",
        "        \"\"\"\n",
        "    ).fetchone()[0]\n",
        "    print(\"\\nDOI_norm duplicate rows:\", f\"{dup_rows:,}\")\n",
        "    print(\"Top duplicated doi_norm:\")\n",
        "    print(con.execute(\n",
        "        \"\"\"\n",
        "        SELECT doi_norm, COUNT(*) AS c\n",
        "        FROM v_doi\n",
        "        WHERE doi_norm <> ''\n",
        "        GROUP BY 1\n",
        "        HAVING COUNT(*) > 1\n",
        "        ORDER BY c DESC\n",
        "        LIMIT 10\n",
        "        \"\"\"\n",
        "    ).fetch_df())\n",
        "else:\n",
        "    print(\"\\n[doi] column not found; skip DOI checks.\")\n",
        "\n",
        "# 3) Year range/distribution (if year-like column exists)\n",
        "year_col = None\n",
        "for cand in ['year', 'pub_year', 'publication_year']:\n",
        "    if cand in cols:\n",
        "        year_col = cand\n",
        "        break\n",
        "\n",
        "if year_col:\n",
        "    print(\"\\nYear stats:\")\n",
        "    print(con.execute(\n",
        "        f\"\"\"\n",
        "        SELECT \n",
        "          MIN(try_cast({q(year_col)} AS INTEGER)) AS min_year,\n",
        "          MAX(try_cast({q(year_col)} AS INTEGER)) AS max_year,\n",
        "          SUM(CASE WHEN {q(year_col)} IS NULL THEN 1 ELSE 0 END) AS null_years\n",
        "        FROM v\n",
        "        \"\"\"\n",
        "    ).fetch_df())\n",
        "\n",
        "    print(\"Out-of-range/unparsable years (should be small or zero):\")\n",
        "    print(con.execute(\n",
        "        f\"\"\"\n",
        "        SELECT COUNT(*) AS bad_years\n",
        "        FROM v\n",
        "        WHERE try_cast({q(year_col)} AS INTEGER) IS NULL\n",
        "           OR try_cast({q(year_col)} AS INTEGER) < 2000\n",
        "           OR try_cast({q(year_col)} AS INTEGER) > 2025\n",
        "        \"\"\"\n",
        "    ).fetch_df())\n",
        "\n",
        "    print(\"Counts by year:\")\n",
        "    print(con.execute(\n",
        "        f\"\"\"\n",
        "        SELECT try_cast({q(year_col)} AS INTEGER) AS y, COUNT(*) AS c\n",
        "        FROM v\n",
        "        GROUP BY 1\n",
        "        ORDER BY 1\n",
        "        \"\"\"\n",
        "    ).fetch_df())\n",
        "else:\n",
        "    print(\"\\n[year] column not found; skip year checks.\")\n",
        "\n",
        "# 4) Key field quality (null rate, distinct counts)\n",
        "key_fields = [c for c in ['title','authors','source','journal','year'] if c in cols]\n",
        "if key_fields:\n",
        "    print(\"\\nKey field quality:\")\n",
        "    for c in key_fields:\n",
        "        df = con.execute(\n",
        "            f\"\"\"\n",
        "            SELECT \n",
        "              COUNT(*)::BIGINT AS total,\n",
        "              SUM(CASE WHEN {q(c)} IS NULL OR {q(c)}='' THEN 1 ELSE 0 END)::BIGINT AS nulls,\n",
        "              COUNT(DISTINCT {q(c)})::BIGINT AS distinct_vals\n",
        "            FROM v\n",
        "            \"\"\"\n",
        "        ).fetch_df()\n",
        "        df['null_ratio'] = (df['nulls'] / df['total']).round(4)\n",
        "        print(f\"[{c}]\"); display(df)\n",
        "else:\n",
        "    print(\"\\nNo key fields to profile.\")\n",
        "\n",
        "# 5) Optional: exact-duplicate rows across all columns (may be slower)\n",
        "try:\n",
        "    all_rows = con.execute(\"SELECT COUNT(*) FROM v\").fetchone()[0]\n",
        "    distinct_rows = con.execute(\"SELECT COUNT(*) FROM (SELECT DISTINCT * FROM v)\").fetchone()[0]\n",
        "    print(\"\\nExact duplicate rows (all-columns):\", f\"{all_rows - distinct_rows:,}\")\n",
        "except Exception as e:\n",
        "    print(\"\\nExact-duplicate check skipped:\", str(e))\n",
        "\n",
        "# Sample a few rows\n",
        "print(\"\\nSample rows from Parquet:\")\n",
        "sample_df = pd.read_parquet(OUTPUT_PARQUET, engine='pyarrow').head(5)\n",
        "sample_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reproducibility Metadata\n",
        "We record the versions of key libraries and basic runtime statistics to support reproducibility.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: 3.11.10\n",
            "Platform: macOS-26.1-arm64-arm-64bit\n",
            "pandas: 2.3.2\n",
            "pyarrow: 21.0.0\n",
            "duckdb: 1.4.0\n",
            "CSV size (MB): 2268.29\n",
            "Parquet size (MB): 695.48\n"
          ]
        }
      ],
      "source": [
        "import platform\n",
        "import sys\n",
        "\n",
        "# Library versions\n",
        "print(\"Python:\", sys.version.split()[0])\n",
        "print(\"Platform:\", platform.platform())\n",
        "print(\"pandas:\", pd.__version__)\n",
        "print(\"pyarrow:\", pa.__version__)\n",
        "print(\"duckdb:\", duckdb.__version__)\n",
        "\n",
        "# Basic file stats\n",
        "if OUTPUT_CSV.exists():\n",
        "    print(\"CSV size (MB):\", round(OUTPUT_CSV.stat().st_size / (1024**2), 2))\n",
        "if OUTPUT_PARQUET.exists():\n",
        "    print(\"Parquet size (MB):\", round(OUTPUT_PARQUET.stat().st_size / (1024**2), 2))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
