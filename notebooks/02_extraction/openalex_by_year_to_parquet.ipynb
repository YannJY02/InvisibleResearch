{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OpenAlex by-year downloader to a single Parquet\n",
        "\n",
        "This notebook fetches all `works` for the Communication subfield (`subfields/3315`) from years 2000â€“2025 using cursor-based paging, and writes them into a single Parquet file at `data/processed/communication_works.parquet`.\n",
        "\n",
        "- Uses `OPENALEX_MAILTO` from environment for the polite pool\n",
        "- Streams page results into Parquet to avoid high memory usage\n",
        "- Includes progress logging and count validation per year\n",
        "- Kernel: Python (InvisibleResearch venv)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Please set environment variable OPENALEX_MAILTO to your contact email.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m CONTACT_EMAIL = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mOPENALEX_MAILTO\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m CONTACT_EMAIL:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPlease set environment variable OPENALEX_MAILTO to your contact email.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m os.makedirs(os.path.dirname(PARQUET_PATH), exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     31\u001b[39m BASE_URL = \u001b[33m\"\u001b[39m\u001b[33mhttps://api.openalex.org/works\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[31mRuntimeError\u001b[39m: Please set environment variable OPENALEX_MAILTO to your contact email."
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import math\n",
        "from datetime import datetime\n",
        "from typing import Dict, Any, Iterator, List, Optional\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "from tqdm import tqdm\n",
        "# Optional: load .env using python-dotenv if available; fallback to manual loader\n",
        "try:\n",
        "    from dotenv import load_dotenv  # type: ignore\n",
        "    load_dotenv()\n",
        "except Exception:\n",
        "    def _simple_load_dotenv(path: str = \".env\") -> None:\n",
        "        if not os.path.exists(path):\n",
        "            return\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for raw in f:\n",
        "                line = raw.strip()\n",
        "                if not line or line.startswith(\"#\"):\n",
        "                    continue\n",
        "                if \"=\" not in line:\n",
        "                    continue\n",
        "                key, val = line.split(\"=\", 1)\n",
        "                key = key.strip()\n",
        "                val = val.strip().strip(\"'\\\"\")\n",
        "                os.environ.setdefault(key, val)\n",
        "    _simple_load_dotenv()\n",
        "\n",
        "# Configuration (fixed as per confirmation)\n",
        "SUBFIELD_ID = \"subfields/3315\"  # Communication\n",
        "START_YEAR = int(os.getenv(\"OPENALEX_START_YEAR\", 2000))\n",
        "END_YEAR = int(os.getenv(\"OPENALEX_END_YEAR\", 2025))\n",
        "PARQUET_PATH = os.path.join(\"data\", \"processed\", \"communication_works.parquet\")\n",
        "PER_PAGE_CANDIDATES = [200, 150, 100]  # try larger first, fallback if needed\n",
        "REQUEST_TIMEOUT = 60\n",
        "RETRY_MAX = 5\n",
        "BACKOFF_BASE = 1.5\n",
        "\n",
        "# Polite pool email from environment\n",
        "CONTACT_EMAIL = os.getenv(\"OPENALEX_MAILTO\")\n",
        "if not CONTACT_EMAIL:\n",
        "    raise RuntimeError(\"Please set environment variable OPENALEX_MAILTO to your contact email.\")\n",
        "\n",
        "os.makedirs(os.path.dirname(PARQUET_PATH), exist_ok=True)\n",
        "\n",
        "BASE_URL = \"https://api.openalex.org/works\"\n",
        "\n",
        "\n",
        "def fetch_page(year: int, cursor: str, per_page: int, mailto: str) -> Dict[str, Any]:\n",
        "    params = {\n",
        "        \"filter\": f\"primary_topic.subfield.id:{SUBFIELD_ID},publication_year:{year}\",\n",
        "        \"per-page\": per_page,\n",
        "        \"cursor\": cursor,\n",
        "        \"mailto\": mailto,\n",
        "    }\n",
        "    resp = requests.get(BASE_URL, params=params, timeout=REQUEST_TIMEOUT)\n",
        "    resp.raise_for_status()\n",
        "    return resp.json()\n",
        "\n",
        "\n",
        "def iterate_year(year: int, per_page: int, mailto: str, sleep_seconds: float = 0.3) -> Iterator[Dict[str, Any]]:\n",
        "    cursor = \"*\"\n",
        "    total_retrieved = 0\n",
        "    meta_count: Optional[int] = None\n",
        "    while True:\n",
        "        data = fetch_page(year, cursor=cursor, per_page=per_page, mailto=mailto)\n",
        "        if meta_count is None:\n",
        "            meta_count = data.get(\"meta\", {}).get(\"count\")\n",
        "            print(f\"Year {year}: expected {meta_count} works with per_page={per_page}\")\n",
        "        results = data.get(\"results\", [])\n",
        "        for work in results:\n",
        "            total_retrieved += 1\n",
        "            yield work\n",
        "        next_cursor = data.get(\"meta\", {}).get(\"next_cursor\")\n",
        "        if not next_cursor:\n",
        "            break\n",
        "        cursor = next_cursor\n",
        "        time.sleep(sleep_seconds)\n",
        "    if meta_count is not None and total_retrieved != meta_count:\n",
        "        print(f\"WARNING: Year {year} mismatch: retrieved {total_retrieved} vs meta.count {meta_count}\")\n",
        "    else:\n",
        "        print(f\"Year {year}: validated {total_retrieved} works\")\n",
        "\n",
        "\n",
        "def robust_iterate_year(year: int, mailto: str) -> Iterator[Dict[str, Any]]:\n",
        "    last_error: Optional[Exception] = None\n",
        "    for per_page in PER_PAGE_CANDIDATES:\n",
        "        for attempt in range(1, RETRY_MAX + 1):\n",
        "            try:\n",
        "                yield from iterate_year(year, per_page=per_page, mailto=mailto)\n",
        "                last_error = None\n",
        "                break\n",
        "            except requests.HTTPError as e:\n",
        "                last_error = e\n",
        "                status = getattr(e.response, \"status_code\", None)\n",
        "                if status in (429, 502, 503, 504):\n",
        "                    delay = BACKOFF_BASE ** attempt\n",
        "                    print(f\"HTTP {status} on year {year}, per_page={per_page}, retry {attempt}/{RETRY_MAX} after {delay:.1f}s...\")\n",
        "                    time.sleep(delay)\n",
        "                    continue\n",
        "                raise\n",
        "            except requests.RequestException as e:\n",
        "                last_error = e\n",
        "                delay = BACKOFF_BASE ** attempt\n",
        "                print(f\"Network error on year {year}, per_page={per_page}, retry {attempt}/{RETRY_MAX} after {delay:.1f}s...\")\n",
        "                time.sleep(delay)\n",
        "                continue\n",
        "        if last_error is None:\n",
        "            return\n",
        "    if last_error is not None:\n",
        "        raise last_error\n",
        "\n",
        "\n",
        "def flatten_work(record: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    # Minimal flattening: keep id, title, publication_year, primary_topic, type, doi, authorship count\n",
        "    out = {\n",
        "        \"id\": record.get(\"id\"),\n",
        "        \"title\": record.get(\"title\"),\n",
        "        \"publication_year\": record.get(\"publication_year\"),\n",
        "        \"type\": record.get(\"type\"),\n",
        "        \"doi\": record.get(\"doi\"),\n",
        "        \"authorships_count\": len(record.get(\"authorships\", []) or []),\n",
        "        \"primary_topic_id\": (record.get(\"primary_topic\") or {}).get(\"id\"),\n",
        "        \"primary_topic_display_name\": (record.get(\"primary_topic\") or {}).get(\"display_name\"),\n",
        "    }\n",
        "    return out\n",
        "\n",
        "\n",
        "start = datetime.now()\n",
        "print(f\"Writing to {PARQUET_PATH}\")\n",
        "writer: Optional[pq.ParquetWriter] = None\n",
        "schema: Optional[pa.schema] = None\n",
        "\n",
        "try:\n",
        "    for year in range(START_YEAR, END_YEAR + 1):\n",
        "        batch_rows: List[Dict[str, Any]] = []\n",
        "        with tqdm(desc=f\"Year {year}\", unit=\"works\") as pbar:\n",
        "            for work in robust_iterate_year(year, mailto=CONTACT_EMAIL):\n",
        "                batch_rows.append(flatten_work(work))\n",
        "                # flush in chunks to manage memory\n",
        "                if len(batch_rows) >= 2000:\n",
        "                    table = pa.Table.from_pylist(batch_rows)\n",
        "                    if writer is None:\n",
        "                        schema = table.schema\n",
        "                        writer = pq.ParquetWriter(PARQUET_PATH, schema=schema)\n",
        "                    writer.write_table(table)\n",
        "                    pbar.update(len(batch_rows))\n",
        "                    batch_rows.clear()\n",
        "            # flush remainder for the year\n",
        "            if batch_rows:\n",
        "                table = pa.Table.from_pylist(batch_rows)\n",
        "                if writer is None:\n",
        "                    schema = table.schema\n",
        "                    writer = pq.ParquetWriter(PARQUET_PATH, schema=schema)\n",
        "                writer.write_table(table)\n",
        "                pbar.update(len(batch_rows))\n",
        "                batch_rows.clear()\n",
        "finally:\n",
        "    if writer is not None:\n",
        "        writer.close()\n",
        "        print(f\"Closed Parquet writer: {PARQUET_PATH}\")\n",
        "    elapsed = datetime.now() - start\n",
        "    print(f\"Completed in {elapsed}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to run\n",
        "\n",
        "1. Ensure your venv is active and the kernel is installed:\n",
        "\n",
        "```bash\n",
        "source /Users/yann.jy/InvisibleResearch/.venv/bin/activate\n",
        "python -m ipykernel install --user --name invisible-research-venv --display-name \"Python (InvisibleResearch venv)\"\n",
        "```\n",
        "\n",
        "2. Set your email for the polite pool in the same shell (no passwords needed):\n",
        "\n",
        "```bash\n",
        "export OPENALEX_MAILTO=\"jinyi.yang@student.uva.nl\"\n",
        "```\n",
        "\n",
        "3. Open the notebook and select kernel \"Python (InvisibleResearch venv)\", then run all cells.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick smoke test: fetch a small sample to verify connectivity and schema\n",
        "import os\n",
        "import requests\n",
        "\n",
        "CONTACT_EMAIL = os.getenv(\"OPENALEX_MAILTO\")\n",
        "params = {\n",
        "    \"filter\": \"primary_topic.subfield.id:subfields/3315,publication_year:2020\",\n",
        "    \"per-page\": 5,\n",
        "    \"cursor\": \"*\",\n",
        "    \"mailto\": CONTACT_EMAIL,\n",
        "}\n",
        "resp = requests.get(\"https://api.openalex.org/works\", params=params, timeout=30)\n",
        "resp.raise_for_status()\n",
        "js = resp.json()\n",
        "print(js.get(\"meta\", {}))\n",
        "print(\"first ids:\", [r.get(\"id\") for r in js.get(\"results\", [])])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Align output columns to CSV schema using OpenAlex select and schema-based flattening\n",
        "import csv\n",
        "from typing import Any\n",
        "\n",
        "CSV_SCHEMA_PATH = os.getenv(\n",
        "    \"CSV_SCHEMA_PATH\",\n",
        "    \"/Users/yann.jy/InvisibleResearch/data/raw/works-2025-09-07T08-08-59.csv\",\n",
        ")\n",
        "\n",
        "# Load column schema from CSV header\n",
        "with open(CSV_SCHEMA_PATH, \"r\", encoding=\"utf-8\") as _f:\n",
        "    _reader = csv.reader(_f)\n",
        "    SCHEMA_COLUMNS = next(_reader)\n",
        "\n",
        "# Derive top-level fields for OpenAlex select to minimize payload\n",
        "# For nested like \"primary_location.source.display_name\" -> select \"primary_location\"\n",
        "TOP_LEVEL_FIELDS = set()\n",
        "for col in SCHEMA_COLUMNS:\n",
        "    if col == \"abstract\":\n",
        "        TOP_LEVEL_FIELDS.add(\"abstract_inverted_index\")\n",
        "        continue\n",
        "    if \".\" in col:\n",
        "        TOP_LEVEL_FIELDS.add(col.split(\".\", 1)[0])\n",
        "    else:\n",
        "        TOP_LEVEL_FIELDS.add(col)\n",
        "# Always include id for joins\n",
        "TOP_LEVEL_FIELDS.add(\"id\")\n",
        "SELECT_PARAM = \",\".join(sorted(TOP_LEVEL_FIELDS))\n",
        "print(f\"Using select with {len(TOP_LEVEL_FIELDS)} top-level fields\")\n",
        "\n",
        "# Override fetch_page to include select\n",
        "\n",
        "def fetch_page(year: int, cursor: str, per_page: int, mailto: str) -> Dict[str, Any]:\n",
        "    params = {\n",
        "        \"filter\": f\"primary_topic.subfield.id:{SUBFIELD_ID},publication_year:{year}\",\n",
        "        \"per-page\": per_page,\n",
        "        \"cursor\": cursor,\n",
        "        \"mailto\": mailto,\n",
        "        \"select\": SELECT_PARAM,\n",
        "    }\n",
        "    resp = requests.get(BASE_URL, params=params, timeout=REQUEST_TIMEOUT)\n",
        "    resp.raise_for_status()\n",
        "    return resp.json()\n",
        "\n",
        "\n",
        "def _flatten_list(values: List[Any]) -> List[Any]:\n",
        "    out: List[Any] = []\n",
        "    for v in values:\n",
        "        if isinstance(v, list):\n",
        "            out.extend(_flatten_list(v))\n",
        "        else:\n",
        "            out.append(v)\n",
        "    return out\n",
        "\n",
        "\n",
        "def extract_path(obj: Any, path: str) -> Any:\n",
        "    # Special handling for abstract reconstructed from abstract_inverted_index\n",
        "    if path == \"abstract\":\n",
        "        inv = obj.get(\"abstract_inverted_index\") if isinstance(obj, dict) else None\n",
        "        if not isinstance(inv, dict):\n",
        "            return None\n",
        "        # Reconstruct abstract text from inverted index\n",
        "        max_pos = -1\n",
        "        for word, positions in inv.items():\n",
        "            if positions:\n",
        "                max_pos = max(max_pos, max(positions))\n",
        "        if max_pos < 0:\n",
        "            return None\n",
        "        words = [None] * (max_pos + 1)\n",
        "        for word, positions in inv.items():\n",
        "            for pos in positions:\n",
        "                if 0 <= pos < len(words) and words[pos] is None:\n",
        "                    words[pos] = word\n",
        "        return \" \".join(w for w in words if isinstance(w, str))\n",
        "\n",
        "    parts = path.split(\".\")\n",
        "    def _walk(current: Any, idx: int) -> Any:\n",
        "        if idx == len(parts):\n",
        "            return current\n",
        "        key = parts[idx]\n",
        "        if isinstance(current, dict):\n",
        "            return _walk(current.get(key), idx + 1)\n",
        "        if isinstance(current, list):\n",
        "            return _flatten_list([_walk(item, idx) for item in current])\n",
        "        return None\n",
        "\n",
        "    return _walk(obj, 0)\n",
        "\n",
        "\n",
        "# Override flatten_work to emit exactly SCHEMA_COLUMNS\n",
        "\n",
        "def flatten_work(record: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    row: Dict[str, Any] = {}\n",
        "    for col in SCHEMA_COLUMNS:\n",
        "        try:\n",
        "            row[col] = extract_path(record, col)\n",
        "        except Exception:\n",
        "            row[col] = None\n",
        "    return row\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
