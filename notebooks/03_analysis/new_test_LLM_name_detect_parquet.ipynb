{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 📊 ArticleInfo Parquet Author Sampling Analysis\n",
        "\n",
        "## 📋 Project Background\n",
        "\n",
        "This notebook implements author name sampling analysis from the **articleInfo.parquet** dataset to examine author name quality and patterns for data cleaning purposes.\n",
        "\n",
        "### 🎯 Analysis Objectives\n",
        "- **Data Source**: `data/processed/articleInfo.parquet` (3.8GB, ~30M records)\n",
        "- **Sampling Strategy**: 10 samples per author count group\n",
        "- **Column Preservation**: All 16 original columns maintained\n",
        "- **Output**: `new_creator_sample.parquet` for manual inspection\n",
        "\n",
        "### 📊 Expected Workflow\n",
        "1. Load ArticleInfo parquet data\n",
        "2. Count authors using delimiter-based splitting\n",
        "3. Sample 10 records per author count group\n",
        "4. Export complete sample data with all columns\n",
        "5. Display sample for manual quality inspection\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ⚙️ Environment Setup & Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Sample 10 rows for every distinct author-count (based on multi-delimiter split),\n",
        "from articleInfo.parquet, preserving ALL original columns.\n",
        "Print samples and write to `new_creator_sample.parquet`.\n",
        "\n",
        "Adapted from: scripts/03_analysis/test_LLM_name_detect_parquet.py\n",
        "\"\"\"\n",
        "\n",
        "from pathlib import Path\n",
        "import random\n",
        "import re\n",
        "import pyarrow.parquet as pq\n",
        "import pyarrow as pa\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✅ Dependencies loaded successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📁 Configuration & File Paths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# File path configuration\n",
        "BASE_DIR = Path().resolve().parent.parent  # Navigate to project root from notebooks/03_analysis\n",
        "PARQUET_PATH = BASE_DIR / \"data/processed/articleInfo.parquet\"\n",
        "OUT_PATH = BASE_DIR / \"data/processed/new_creator_sample.parquet\"\n",
        "MAX_SAMPLES = 10\n",
        "\n",
        "# Regex for author delimiters EXCLUDING comma (',' may appear in Last, First)\n",
        "DELIM_RE = re.compile(r\"\\s*(?:;|&|\\band\\b|\\+|/|\\\\)\\s*\", flags=re.IGNORECASE)\n",
        "\n",
        "print(f\"📂 Project root: {BASE_DIR}\")\n",
        "print(f\"📊 Source parquet: {PARQUET_PATH}\")\n",
        "print(f\"💾 Output parquet: {OUT_PATH}\")\n",
        "print(f\"🔢 Max samples per group: {MAX_SAMPLES}\")\n",
        "print(f\"🔍 Author delimiter regex: {DELIM_RE.pattern}\")\n",
        "\n",
        "# Verify source file exists\n",
        "if not PARQUET_PATH.exists():\n",
        "    raise SystemExit(f\"❌ Parquet file not found: {PARQUET_PATH}\")\n",
        "else:\n",
        "    print(f\"✅ Source file verified: {PARQUET_PATH.name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📊 Data Loading & Author Count Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load complete dataset preserving all columns\n",
        "print(f\"🚀 Loading dataset from {PARQUET_PATH.name} ...\")\n",
        "\n",
        "# Read all columns to preserve complete structure\n",
        "table = pq.read_table(PARQUET_PATH)\n",
        "df = table.to_pandas()\n",
        "\n",
        "print(f\"✅ Dataset loaded successfully:\")\n",
        "print(f\"  📊 Shape: {df.shape}\")\n",
        "print(f\"  📋 Columns: {list(df.columns)}\")\n",
        "\n",
        "# Verify authors column exists\n",
        "if 'authors' not in df.columns:\n",
        "    print(\"❌ 'authors' column not found!\")\n",
        "    print(f\"Available columns: {list(df.columns)}\")\n",
        "    raise SystemExit(\"Missing 'authors' column\")\n",
        "\n",
        "# Show sample data\n",
        "print(f\"\\n🔬 Sample data (first 3 rows):\")\n",
        "display_cols = ['id', 'title1', 'authors', 'year']\n",
        "if all(col in df.columns for col in display_cols):\n",
        "    display(df[display_cols].head(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute author count using delimiter-based splitting\n",
        "print(\"🧮 Computing author counts...\")\n",
        "\n",
        "# Count authors by splitting on delimiters (; & 'and' + / \\) – comma is ignored\n",
        "df[\"author_count\"] = (\n",
        "    df[\"authors\"]\n",
        "      .fillna(\"\")            # avoid NaN\n",
        "      .astype(str)\n",
        "      .apply(\n",
        "          lambda x: len([p for p in DELIM_RE.split(x) if p.strip()]) if x.strip() else 0\n",
        "      )\n",
        ")\n",
        "\n",
        "# Analyze author count distribution\n",
        "author_count_dist = df['author_count'].value_counts().sort_index()\n",
        "\n",
        "print(f\"\\n📊 Author Count Distribution (top 10):\")\n",
        "for count, freq in author_count_dist.head(10).items():\n",
        "    print(f\"  {count:2d} authors: {freq:,} papers ({freq/len(df)*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\n📈 Summary Statistics:\")\n",
        "print(f\"  📊 Mean authors per paper: {df['author_count'].mean():.2f}\")\n",
        "print(f\"  📊 Median authors per paper: {df['author_count'].median():.1f}\")\n",
        "print(f\"  📊 Max authors per paper: {df['author_count'].max()}\")\n",
        "print(f\"  📊 Papers with 0 authors: {(df['author_count'] == 0).sum():,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎯 Stratified Sampling & Sample Display\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Group and sample by author count (same logic as original script)\n",
        "print(\"🎯 Performing stratified sampling by author count...\")\n",
        "\n",
        "results = []\n",
        "sample_frames = []\n",
        "\n",
        "for count, grp in df.groupby(\"author_count\"):\n",
        "    if count == 0:       # skip empty rows for display but keep in sample\n",
        "        continue\n",
        "        \n",
        "    sample_n = min(len(grp), MAX_SAMPLES)\n",
        "    sample_df = grp.sample(sample_n, random_state=0).copy()\n",
        "    sample_frames.append(sample_df)          # for parquet output\n",
        "\n",
        "    # gather tuples for pretty printing (authors column only for display)\n",
        "    sample_tuples = list(sample_df[['authors']].itertuples(index=False, name=None))\n",
        "    results.append((count, sample_tuples))\n",
        "\n",
        "print(f\"✅ Sampling completed: {len(sample_frames)} groups processed\")\n",
        "\n",
        "# Print to console for manual inspection\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"👁️ SAMPLE DATA FOR MANUAL INSPECTION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for count, authors_list in sorted(results):\n",
        "    print(f\"\\n📝 Author count = {count} (showing {len(authors_list)} samples)\")\n",
        "    print(\"-\" * 50)\n",
        "    for i, (author_str,) in enumerate(authors_list, 1):\n",
        "        print(f\"{i:2d}. {author_str}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 💾 Export Complete Sample with All Columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write sample parquet preserving ALL original columns\n",
        "if sample_frames:\n",
        "    sample_df_total = pd.concat(sample_frames, ignore_index=True)\n",
        "    \n",
        "    # Ensure Arrow engine for compatibility\n",
        "    sample_df_total.to_parquet(OUT_PATH, index=False, engine=\"pyarrow\")\n",
        "    \n",
        "    print(f\"✅ Sample parquet written successfully:\")\n",
        "    print(f\"  📁 File: {OUT_PATH}\")\n",
        "    print(f\"  📊 Records: {len(sample_df_total):,}\")\n",
        "    print(f\"  📋 Columns: {len(sample_df_total.columns)} (all original columns preserved)\")\n",
        "    print(f\"  💾 File size: {OUT_PATH.stat().st_size / 1024:.1f} KB\")\n",
        "    \n",
        "    print(f\"\\n📋 All preserved columns:\")\n",
        "    for i, col in enumerate(sample_df_total.columns, 1):\n",
        "        print(f\"  {i:2d}. {col}\")\n",
        "    \n",
        "    print(f\"\\n🔬 Sample verification (showing key columns):\")\n",
        "    key_cols = ['id', 'title1', 'authors', 'author_count', 'year', 'publisher1']\n",
        "    available_key_cols = [col for col in key_cols if col in sample_df_total.columns]\n",
        "    if available_key_cols:\n",
        "        display(sample_df_total[available_key_cols].head(5))\n",
        "    \n",
        "else:\n",
        "    print(\"⚠️  No non-empty author rows found; sample parquet not created.\")\n",
        "\n",
        "print(f\"\\n🎉 Analysis completed! Check {OUT_PATH.name} for complete sample data.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
