{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dimension-derived Variable Construction (English-only)\n",
        "\n",
        "This notebook derives analysis-ready variables from the unified Dimensions dataset and writes a single Parquet for downstream analyses. The workflow adheres to academic reporting best practices: each variable is introduced by a short rationale (markdown) followed by a reproducible code cell. No intermediate files are written; only the final dataset is saved.\n",
        "\n",
        "- Source dataset: `/Users/yann.jy/InvisibleResearch/data/processed/dimension_merged.parquet`\n",
        "- External reference (for disciplinary core journal matching): `/Users/yann.jy/InvisibleResearch/data/processed/scimagojr_communication_journal_1999_2024.csv`\n",
        "- Output dataset: `/Users/yann.jy/InvisibleResearch/data/processed/dimension_data_for_analysis.parquet`\n",
        "\n",
        "Conventions:\n",
        "- All module outputs include `id` for reliable merges.\n",
        "- Column blocks are ordered by conceptual variable groups so related columns appear together in the final file.\n",
        "- Time-window logic for invisibility is intentionally not applied here; analyses may add temporal filters later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input exists: True\n",
            "SJR CSV exists: True\n",
            "Output path: /Users/yann.jy/InvisibleResearch/data/processed/dimension_data_for_analysis.parquet\n"
          ]
        }
      ],
      "source": [
        "# Setup and paths\n",
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "# Paths (absolute as requested)\n",
        "PROJECT_ROOT = Path('/Users/yann.jy/InvisibleResearch')\n",
        "PARQUET_INPUT = PROJECT_ROOT / 'data/processed/dimension_merged.parquet'\n",
        "SJR_CSV = PROJECT_ROOT / 'data/processed/scimagojr_communication_journal_1999_2024.csv'\n",
        "OUTPUT_PARQUET = PROJECT_ROOT / 'data/processed/dimension_data_for_analysis.parquet'\n",
        "\n",
        "pd.set_option('display.max_colwidth', 160)\n",
        "pd.set_option('display.width', 160)\n",
        "\n",
        "print('Input exists:', PARQUET_INPUT.exists())\n",
        "print('SJR CSV exists:', SJR_CSV.exists())\n",
        "print('Output path:', OUTPUT_PARQUET)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Schema preview\n",
        "\n",
        "We first inspect the Parquet schema and a small sample to confirm column availability and obtain a quick sense of the data. No full-table prints are performed to avoid large outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of columns: 76\n",
            "Columns:\n",
            "['abstract', 'acknowledgements', 'altmetric', 'altmetric_id', 'arxiv_id', 'authors', 'authors_count', 'book_doi', 'book_series_title', 'book_title', 'category_bra', 'category_for', 'category_for_2020', 'category_hra', 'category_hrcs_hc', 'category_hrcs_rac', 'category_icrp_cso', 'category_icrp_ct', 'category_rcdc', 'category_sdg', 'category_uoa', 'clinical_trial_ids', 'concepts', 'concepts_scores', 'date', 'date_inserted', 'date_online', 'date_print', 'dimensions_url', 'document_type', 'doi', 'editors', 'field_citation_ratio', 'funder_countries', 'funders', 'funding_section', 'id', 'isbn', 'issn', 'issue', 'journal.id', 'journal.title', 'journal_lists', 'journal_title_raw', 'linkout', 'mesh_terms', 'open_access', 'pages', 'pmcid', 'pmid', 'proceedings_title', 'publisher', 'recent_citations', 'reference_ids', 'referenced_pubs', 'relative_citation_ratio', 'research_org_cities', 'research_org_countries', 'research_org_country_names', 'research_org_names', 'research_org_state_codes', 'research_org_state_names', 'research_org_types', 'research_orgs', 'researchers', 'resulting_publication_doi', 'score', 'source_title.id', 'source_title.title', 'subtitles', 'supporting_grant_ids', 'times_cited', 'title', 'type', 'volume', 'year']\n",
            "Total rows (metadata): 358493\n",
            "\n",
            "Head (subset of columns):\n",
            "                                                                                                                                                          abstract  \\\n",
            "0  <p>Coverage of the Clinton-Lewinsky saga followed in a long trail of media exposures of the more personal details of the lives of public figures. Many comme...   \n",
            "1  <p>Coverage of the Clinton-Lewinsky saga followed in a long trail of media exposures of the more personal details of the lives of public figures. Many comme...   \n",
            "2  <p>Coverage of the Clinton-Lewinsky saga followed in a long trail of media exposures of the more personal details of the lives of public figures. Many comme...   \n",
            "3  <p>Coverage of the Clinton-Lewinsky saga followed in a long trail of media exposures of the more personal details of the lives of public figures. Many comme...   \n",
            "4  <p>Coverage of the Clinton-Lewinsky saga followed in a long trail of media exposures of the more personal details of the lives of public figures. Many comme...   \n",
            "\n",
            "  acknowledgements altmetric altmetric_id arxiv_id authors authors_count               book_doi book_series_title     book_title category_bra  \\\n",
            "0             None      None          0.0     None    None             0  10.5771/9781461643852              None  Tabloid Tales         None   \n",
            "1             None      None          0.0     None    None             0  10.5771/9781461643852              None  Tabloid Tales         None   \n",
            "2             None      None          0.0     None    None             0  10.5771/9781461643852              None  Tabloid Tales         None   \n",
            "3             None      None          0.0     None    None             0  10.5771/9781461643852              None  Tabloid Tales         None   \n",
            "4             None      None          0.0     None    None             0  10.5771/9781461643852              None  Tabloid Tales         None   \n",
            "\n",
            "                                                                                                                                                      category_for  \n",
            "0  [{'id': '80007', 'name': '36 Creative Arts and Writing'}, {'id': '80194', 'name': '4701 Communication and Media Studies'}, {'id': '80018', 'name': '47 Langu...  \n",
            "1  [{'id': '80007', 'name': '36 Creative Arts and Writing'}, {'id': '80194', 'name': '4701 Communication and Media Studies'}, {'id': '80018', 'name': '47 Langu...  \n",
            "2  [{'id': '80007', 'name': '36 Creative Arts and Writing'}, {'id': '80194', 'name': '4701 Communication and Media Studies'}, {'id': '80018', 'name': '47 Langu...  \n",
            "3  [{'id': '80007', 'name': '36 Creative Arts and Writing'}, {'id': '80194', 'name': '4701 Communication and Media Studies'}, {'id': '80018', 'name': '47 Langu...  \n",
            "4  [{'id': '80007', 'name': '36 Creative Arts and Writing'}, {'id': '80194', 'name': '4701 Communication and Media Studies'}, {'id': '80018', 'name': '47 Langu...  \n"
          ]
        }
      ],
      "source": [
        "# Read schema (column names) and show a small preview\n",
        "pf = pq.ParquetFile(PARQUET_INPUT)\n",
        "print('Number of columns:', len(pf.schema.names))\n",
        "print('Columns:')\n",
        "print(pf.schema.names)\n",
        "\n",
        "# Lightweight row count and a tiny data preview\n",
        "num_rows = pf.metadata.num_rows\n",
        "print('Total rows (metadata):', num_rows)\n",
        "\n",
        "# Sample a few rows without loading entire file (avoid unsupported filters arg)\n",
        "base_head = pd.read_parquet(PARQUET_INPUT, engine='pyarrow', columns=pf.schema.names[:12]).head(5)\n",
        "print('\\nHead (subset of columns):')\n",
        "print(base_head)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Variable: Invisibility (times_cited only)\n",
        "\n",
        "Definition: A record is labeled as invisible if it has received zero citations, i.e., `invisibility = 1` when `times_cited == 0`; otherwise `0`. We retain `date` for analytical context but do not use it in the rule here. All outputs include `id` for merging.\n",
        "\n",
        "Columns used: `id`, `times_cited`, `date`.\n",
        "\n",
        "Output columns: `id`, `invisibility`, `times_cited`, `date`. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               id  invisibility times_cited        date\n",
            "0  pub.1186290333             1           0  2000-01-01\n",
            "1  pub.1186290332             1           0  2000-01-01\n",
            "2  pub.1186290331             1           0  2000-01-01\n",
            "3  pub.1186290329             1           0  2000-01-01\n",
            "4  pub.1186290328             1           0  2000-01-01\n",
            "invisibility\n",
            "0       186551\n",
            "1       171940\n",
            "<NA>         2\n",
            "Name: invisibility_counts, dtype: Int64\n"
          ]
        }
      ],
      "source": [
        "# Build invisibility (robust to non-numeric times_cited)\n",
        "base = pd.read_parquet(PARQUET_INPUT, engine='pyarrow', columns=['id','times_cited','date'])\n",
        "inv = base[['id','times_cited','date']].copy()\n",
        "# Convert to numeric, coercing invalid tokens (e.g., stray dict fragments) to NaN\n",
        "_tc = pd.to_numeric(inv['times_cited'], errors='coerce')\n",
        "# Initialize as NA; then fill where numeric is available\n",
        "inv['invisibility'] = pd.Series(pd.NA, index=inv.index, dtype='Int8')\n",
        "mask_num = _tc.notna()\n",
        "inv.loc[mask_num, 'invisibility'] = (_tc.loc[mask_num] == 0).astype('int8')\n",
        "inv['invisibility'] = inv['invisibility'].astype('Int8')\n",
        "# Order output columns as specified\n",
        "inv = inv[['id','invisibility','times_cited','date']]\n",
        "print(inv.head())\n",
        "print(inv['invisibility'].value_counts(dropna=False).rename('invisibility_counts'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Variable: Geographic (institutional location)\n",
        "\n",
        "We retain institutional location fields to support geographic analyses. No transformation is applied here; the values may contain multiple institutions and countries per record.\n",
        "\n",
        "Columns used: `id`, `research_org_country_names`, `research_org_names`, `research_org_types`.\n",
        "\n",
        "Output columns: `id`, `research_org_country_names`, `research_org_names`, `research_org_types`. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               id research_org_country_names research_org_names research_org_types\n",
            "0  pub.1186290333                       None               None               None\n",
            "1  pub.1186290332                       None               None               None\n",
            "2  pub.1186290331                       None               None               None\n",
            "3  pub.1186290329                       None               None               None\n",
            "4  pub.1186290328                       None               None               None\n"
          ]
        }
      ],
      "source": [
        "# Keep geographic fields\n",
        "geo_cols = ['id','research_org_country_names','research_org_names','research_org_types']\n",
        "existing_geo = [c for c in geo_cols if c in pf.schema.names]\n",
        "geo = pd.read_parquet(PARQUET_INPUT, engine='pyarrow', columns=existing_geo)\n",
        "print(geo.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Variable: Topical (concepts)\n",
        "\n",
        "We retain topic-related fields to enable later construction of mainstream-topic shares or binary mainstream indicators. No transformation is performed here.\n",
        "\n",
        "Columns used: `id`, `concepts`, `concepts_scores`.\n",
        "\n",
        "Output columns: `id`, `concepts`, `concepts_scores`. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               id  \\\n",
            "0  pub.1186290333   \n",
            "1  pub.1186290332   \n",
            "2  pub.1186290331   \n",
            "3  pub.1186290329   \n",
            "4  pub.1186290328   \n",
            "\n",
            "                                                                                                                                                          concepts  \\\n",
            "0  ['mass media', 'news habits', 'mass communication', 'public figures', 'media exposure', 'scholarly studies', 'private life', 'tabloids', 'news', 'U.S. probl...   \n",
            "1  ['mass media', 'news habits', 'mass communication', 'public figures', 'media exposure', 'tabloids', 'scholarly studies', 'private life', 'U.S. problems', 'n...   \n",
            "2  ['mass media', 'news habits', 'mass communication', 'public figures', 'media exposure', 'scholarly studies', 'private life', 'U.S. problems', 'tabloids', 'n...   \n",
            "3  ['mass media', 'tabloid journalism', 'news habits', 'mass communication', 'public figures', 'media exposure', 'scholarly studies', 'tabloids', 'private life...   \n",
            "4  ['mass media', 'tabloid journalism', 'news habits', 'mass communication', 'public figures', 'media exposure', 'scholarly studies', 'tabloids', 'private life...   \n",
            "\n",
            "                                                                                                                                                   concepts_scores  \n",
            "0  [{'concept': 'mass media', 'relevance': 0.798}, {'concept': 'news habits', 'relevance': 0.701}, {'concept': 'mass communication', 'relevance': 0.695}, {'con...  \n",
            "1  [{'concept': 'mass media', 'relevance': 0.798}, {'concept': 'news habits', 'relevance': 0.701}, {'concept': 'mass communication', 'relevance': 0.695}, {'con...  \n",
            "2  [{'concept': 'mass media', 'relevance': 0.798}, {'concept': 'news habits', 'relevance': 0.701}, {'concept': 'mass communication', 'relevance': 0.695}, {'con...  \n",
            "3  [{'concept': 'mass media', 'relevance': 0.798}, {'concept': 'tabloid journalism', 'relevance': 0.701}, {'concept': 'news habits', 'relevance': 0.701}, {'con...  \n",
            "4  [{'concept': 'mass media', 'relevance': 0.798}, {'concept': 'tabloid journalism', 'relevance': 0.701}, {'concept': 'news habits', 'relevance': 0.701}, {'con...  \n"
          ]
        }
      ],
      "source": [
        "# Keep topical fields\n",
        "top_cols = ['id','concepts','concepts_scores']\n",
        "existing_top = [c for c in top_cols if c in pf.schema.names]\n",
        "top = pd.read_parquet(PARQUET_INPUT, engine='pyarrow', columns=existing_top)\n",
        "print(top.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Variable: Disciplinary (core journal hit via SJR)\n",
        "\n",
        "We construct a binary indicator for whether the record's journal identifiers intersect the SJR communication journals ISSN list. The variable equals 1 if any standardized token from `issn` or `isbn` matches any SJR `Issn`; otherwise 0.\n",
        "\n",
        "Columns used: `id`, `issn`, `isbn` (from Dimensions) and `Issn` (from SJR CSV).\n",
        "\n",
        "Output columns: `id`, `issn`, `isbn`, `disciplinary`.\n",
        "\n",
        "Normalization policy:\n",
        "- Uppercase tokens; drop hyphens/whitespace; retain only digits and `X`.\n",
        "- Support multi-valued identifiers split by common non-alphanumerics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               id  issn               isbn  disciplinary\n",
            "0  pub.1186290333  None  ['9781461643852']             0\n",
            "1  pub.1186290332  None  ['9781461643852']             0\n",
            "2  pub.1186290331  None  ['9781461643852']             0\n",
            "3  pub.1186290329  None  ['9781461643852']             0\n",
            "4  pub.1186290328  None  ['9781461643852']             0\n",
            "disciplinary\n",
            "0    356157\n",
            "1      2336\n",
            "Name: disciplinary_counts, dtype: Int64\n"
          ]
        }
      ],
      "source": [
        "# Build disciplinary via SJR ISSN match (robust to multi-valued fields)\n",
        "import re\n",
        "\n",
        "# Base identifiers\n",
        "disc_cols = ['id','issn','isbn']\n",
        "existing_disc = [c for c in disc_cols if c in pf.schema.names]\n",
        "disc = pd.read_parquet(PARQUET_INPUT, engine='pyarrow', columns=existing_disc)\n",
        "\n",
        "# --- Utilities ---\n",
        "TOK = re.compile(r'[0-9A-Za-z]+')\n",
        "\n",
        "def issn_tokens(val: object) -> set[str]:\n",
        "    \"\"\"Extract standardized ISSN-like tokens (length==8, digits/X) from any string-ish value.\n",
        "    Handles multi-valued cases separated by commas/semicolons/spaces/brackets/quotes/hyphens.\n",
        "    \"\"\"\n",
        "    if pd.isna(val):\n",
        "        return set()\n",
        "    # split into alnum chunks, then strip to [0-9X], uppercase\n",
        "    raw = TOK.findall(str(val))\n",
        "    cleaned = [re.sub(r'[^0-9X]', '', t.upper()) for t in raw]\n",
        "    # keep only 8-char tokens (ISSN normalized without hyphen)\n",
        "    return {t for t in cleaned if len(t) == 8}\n",
        "\n",
        "# --- Build SJR ISSN set (SJR side may also contain comma-separated values) ---\n",
        "sjr = pd.read_csv(SJR_CSV, dtype=str, usecols=['Issn'])\n",
        "sjr_issn_set: set[str] = set()\n",
        "for s in sjr['Issn'].astype(str):\n",
        "    sjr_issn_set.update(issn_tokens(s))\n",
        "\n",
        "# --- Match against SJR set ---\n",
        "# Prefer explicit column checks for clarity\n",
        "has_issn = 'issn' in disc.columns\n",
        "has_isbn = 'isbn' in disc.columns\n",
        "\n",
        "issn_hit = pd.Series(False, index=disc.index)\n",
        "if has_issn:\n",
        "    issn_hit = disc['issn'].map(issn_tokens).apply(lambda s: any(t in sjr_issn_set for t in s))\n",
        "\n",
        "isbn_hit = pd.Series(False, index=disc.index)\n",
        "if has_isbn:\n",
        "    # ISBN tokens typically not length 8; this filter ensures only ISSN-like tokens can match\n",
        "    isbn_hit = disc['isbn'].map(issn_tokens).apply(lambda s: any(t in sjr_issn_set for t in s))\n",
        "\n",
        "disc['disciplinary'] = (issn_hit | isbn_hit).astype('Int8')\n",
        "print(disc.head())\n",
        "print(disc['disciplinary'].value_counts(dropna=False).rename('disciplinary_counts'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Variable: Prestige (institutional ranking category)\n",
        "\n",
        "We derive a prestige category by fuzzy-matching `research_org_names` against SCImago Institution Rankings (communication), then mapping rank to bins:\n",
        "- Elite: top 100\n",
        "- High: 101–500\n",
        "- Medium: 501–1000\n",
        "- Low: Unranked or no confident match\n",
        "\n",
        "Inputs:\n",
        "- `research_org_names` (may contain multiple institutions per record)\n",
        "- Rankings CSV: `data/processed/scimagoir_2025_Overall Rank_Communication.csv` (expects columns like `institution` and an overall-rank column)\n",
        "\n",
        "Output columns: `id`, `prestige`. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               id  best_rank matched_institution matched_token match_type  match_score prestige\n",
            "0  pub.1186290333        NaN                None          None       None          NaN  Unknown\n",
            "1  pub.1186290332        NaN                None          None       None          NaN  Unknown\n",
            "2  pub.1186290331        NaN                None          None       None          NaN  Unknown\n",
            "3  pub.1186290329        NaN                None          None       None          NaN  Unknown\n",
            "4  pub.1186290328        NaN                None          None       None          NaN  Unknown\n"
          ]
        }
      ],
      "source": [
        "# Build prestige from SCImagoIR 2025 (Communication) by fuzzy-matching (rapidfuzz-accelerated)\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# Ensure rapidfuzz is available in the current kernel\n",
        "try:\n",
        "    from rapidfuzz import process as rf_process, fuzz as rf_fuzz\n",
        "except ModuleNotFoundError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'rapidfuzz'])\n",
        "    from rapidfuzz import process as rf_process, fuzz as rf_fuzz\n",
        "\n",
        "# Tunable fuzzy threshold (validated by sampling)\n",
        "PRESTIGE_FUZZY_THRESHOLD = 0.92\n",
        "SCORE_CUTOFF = int(PRESTIGE_FUZZY_THRESHOLD * 100)\n",
        "\n",
        "rank_csv = PROJECT_ROOT / 'data/processed/scimagoir_2025_Overall Rank_Communication.csv'\n",
        "# Robust CSV reading with explicit columns and trailing placeholder to avoid misalignment\n",
        "rank_df_raw = pd.read_csv(\n",
        "    rank_csv,\n",
        "    dtype=str,\n",
        "    sep=';',\n",
        "    engine='python',\n",
        "    header=0,\n",
        "    names=['Global Rank', 'Institution', 'Country', 'Sector', '_extra']\n",
        ")\n",
        "rank_df_raw = rank_df_raw.drop(columns=['_extra'], errors='ignore')\n",
        "\n",
        "# Use fixed columns from SCImagoIR export\n",
        "name_col = 'Institution'\n",
        "rank_col = 'Global Rank'\n",
        "\n",
        "# Normalize institution names and numeric rank\n",
        "\n",
        "def norm_text(x: object) -> str:\n",
        "    s = str(x) if pd.notna(x) else ''\n",
        "    s = s.lower().strip()\n",
        "    s = re.sub(r\"[\\-–—'’`\\\"]\", ' ', s)\n",
        "    s = re.sub(r\"[^a-z0-9&\\s]\", ' ', s)\n",
        "    s = re.sub(r\"\\s+\", ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "rank_df = rank_df_raw[[name_col, rank_col]].copy()\n",
        "rank_df[name_col] = rank_df[name_col].map(norm_text)\n",
        "# Drop empty institution names explicitly (treat empty string as missing)\n",
        "rank_df = rank_df[rank_df[name_col].astype(str).str.strip() != '']\n",
        "rank_df[rank_col] = pd.to_numeric(rank_df[rank_col], errors='coerce')\n",
        "\n",
        "# Build lookup: normalized name → (best rank, original Institution)\n",
        "rank_best = (\n",
        "    rank_df.join(rank_df_raw[[name_col]], rsuffix='_orig')\n",
        "            .dropna(subset=[rank_col])\n",
        "            .sort_values(rank_col)\n",
        "            .drop_duplicates(subset=[name_col], keep='first')\n",
        ")\n",
        "rank_map = rank_best.set_index(name_col)[rank_col].to_dict()\n",
        "orig_by_norm = rank_best.set_index(name_col)[f'{name_col}_orig'].to_dict()\n",
        "choices = list(rank_map.keys())\n",
        "\n",
        "# Read research_org_names from the geographic frame (may contain multi-values)\n",
        "geo_names = pd.read_parquet(PARQUET_INPUT, engine='pyarrow', columns=['id','research_org_names'])\n",
        "\n",
        "\n",
        "def split_names(val: object) -> list[str]:\n",
        "    if pd.isna(val):\n",
        "        return []\n",
        "    # Split by commas/semicolons/slashes/pipes and collapse bracket/quote noise\n",
        "    s = str(val)\n",
        "    # Replace brackets/quotes with space\n",
        "    s = re.sub(r\"[\\[\\]\\(\\)\\{\\}\\'\\\"]\", ' ', s)\n",
        "    # Then split on common delimiters or 2+ spaces\n",
        "    tokens = re.split(r\"[;,/\\|]|\\s{2,}\", s)\n",
        "    # Drop empties and very short tokens\n",
        "    return [t.strip() for t in tokens if t and t.strip()]\n",
        "\n",
        "# Build unique normalized tokens once\n",
        "unique_tokens: set[str] = set()\n",
        "for val in geo_names['research_org_names']:\n",
        "    if pd.isna(val) or str(val).strip() == '':\n",
        "        continue\n",
        "    for t in split_names(val):\n",
        "        qn = norm_text(t)\n",
        "        if qn:\n",
        "            unique_tokens.add(qn)\n",
        "\n",
        "# Map tokens → meta (rank, original matched institution, score, type, token)\n",
        "token_to_meta: dict[str, dict] = {}\n",
        "for tok in unique_tokens:\n",
        "    if tok in rank_map:\n",
        "        token_to_meta[tok] = {\n",
        "            'rank': float(rank_map[tok]),\n",
        "            'choice': orig_by_norm.get(tok, tok),\n",
        "            'score': 100.0,\n",
        "            'match_type': 'exact',\n",
        "            'token': tok,\n",
        "        }\n",
        "        continue\n",
        "    match = rf_process.extractOne(tok, choices, scorer=rf_fuzz.ratio, score_cutoff=SCORE_CUTOFF)\n",
        "    if match is None:\n",
        "        token_to_meta[tok] = {\n",
        "            'rank': None, 'choice': None, 'score': None, 'match_type': 'none', 'token': tok\n",
        "        }\n",
        "    else:\n",
        "        choice_norm, score, _ = match\n",
        "        token_to_meta[tok] = {\n",
        "            'rank': float(rank_map[choice_norm]),\n",
        "            'choice': orig_by_norm.get(choice_norm, choice_norm),\n",
        "            'score': float(score),\n",
        "            'match_type': 'fuzzy',\n",
        "            'token': tok,\n",
        "        }\n",
        "\n",
        "# For each row, compute best candidate by (rank asc, score desc)\n",
        "rows = []\n",
        "for rid, names in geo_names[['id','research_org_names']].itertuples(index=False):\n",
        "    candidates = []\n",
        "    for n in split_names(names):\n",
        "        qn = norm_text(n)\n",
        "        if not qn:\n",
        "            continue\n",
        "        m = token_to_meta.get(qn)\n",
        "        if m is not None and (m['rank'] is not None):\n",
        "            candidates.append(m)\n",
        "    if candidates:\n",
        "        best = sorted(candidates, key=lambda m: (m['rank'], - (m['score'] or 0.0)))[0]\n",
        "        rows.append((rid, best['rank'], best['choice'], best['token'], best['match_type'], best['score']))\n",
        "    else:\n",
        "        rows.append((rid, None, None, None, None, None))\n",
        "\n",
        "prest_df = pd.DataFrame(rows, columns=['id','best_rank','matched_institution','matched_token','match_type','match_score'])\n",
        "\n",
        "# Map to bins (Unknown for unmatched/missing)\n",
        "\n",
        "def to_prestige(r: float | None) -> str:\n",
        "    if r is None or (isinstance(r, float) and np.isnan(r)):\n",
        "        return 'Unknown'\n",
        "    r = float(r)\n",
        "    if r <= 100:\n",
        "        return 'Elite'\n",
        "    if r <= 500:\n",
        "        return 'High'\n",
        "    if r <= 1000:\n",
        "        return 'Medium'\n",
        "    return 'Low'\n",
        "\n",
        "prest_df['prestige'] = prest_df['best_rank'].map(to_prestige)\n",
        "prest = prest_df[['id','best_rank','matched_institution','matched_token','match_type','match_score','prestige']]\n",
        "print(prest.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Variable: Open Access status\n",
        "\n",
        "We retain the open access indicator as-is for subsequent stratified analyses.\n",
        "\n",
        "Columns used: `id`, `open_access`.\n",
        "\n",
        "Output columns: `id`, `open_access`. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "open_access\n",
            "['closed']              207616\n",
            "['oa_all', 'gold']       74537\n",
            "['oa_all', 'green']      26712\n",
            "['oa_all', 'hybrid']     26096\n",
            "['oa_all', 'bronze']     23530\n",
            "Name: count, dtype: int64\n",
            "               id open_access\n",
            "0  pub.1186290333  ['closed']\n",
            "1  pub.1186290332  ['closed']\n",
            "2  pub.1186290331  ['closed']\n",
            "3  pub.1186290329  ['closed']\n",
            "4  pub.1186290328  ['closed']\n"
          ]
        }
      ],
      "source": [
        "# Keep OA field\n",
        "oa_cols = ['id','open_access']\n",
        "existing_oa = [c for c in oa_cols if c in pf.schema.names]\n",
        "oa = pd.read_parquet(PARQUET_INPUT, engine='pyarrow', columns=existing_oa)\n",
        "print(oa['open_access'].value_counts(dropna=False).head())\n",
        "print(oa.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Variable: First author experience\n",
        "\n",
        "Definition: The difference between this paper's publication year and the first author's earliest publication year observed within this dataset. The goal is to estimate the first author's academic experience at the time of publication.\n",
        "\n",
        "- Identification of first author: prefer stable IDs; we do not use name-based matching to avoid collisions\n",
        "  - Priority: researchers[0].id → authors[0].id; if neither is available, the value remains missing\n",
        "- Year source: prefer `year`; fallback to extracting the year from `date`\n",
        "- Output columns: `id`, `first_author_experience`\n",
        "- Debug-only (not merged into final output): `first_author_key`, `first_author_first_year`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'pf' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m cols = [\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33myear\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mresearchers\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mauthors\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m exist = \u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      7\u001b[39m fa_raw = pd.read_parquet(PARQUET_INPUT, engine=\u001b[33m'\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m'\u001b[39m, columns=exist)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Derive paper_year: prefer `year`, fallback to year parsed from `date`\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m cols = [\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33myear\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mresearchers\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mauthors\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m exist = [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpf\u001b[49m.schema.names]\n\u001b[32m      7\u001b[39m fa_raw = pd.read_parquet(PARQUET_INPUT, engine=\u001b[33m'\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m'\u001b[39m, columns=exist)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Derive paper_year: prefer `year`, fallback to year parsed from `date`\u001b[39;00m\n",
            "\u001b[31mNameError\u001b[39m: name 'pf' is not defined"
          ]
        }
      ],
      "source": [
        "# Compute first_author_experience (difference between paper year and first author's first-year)\n",
        "import json, ast, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure schema access even if previous cells not executed\n",
        "try:\n",
        "    _schema_names = pf.schema.names  # reuse if available\n",
        "except NameError:\n",
        "    import pyarrow.parquet as pq\n",
        "    _schema_names = pq.ParquetFile(PARQUET_INPUT).schema.names\n",
        "\n",
        "cols = ['id','year','date','researchers','authors']\n",
        "exist = [c for c in cols if c in _schema_names]\n",
        "fa_raw = pd.read_parquet(PARQUET_INPUT, engine='pyarrow', columns=exist)\n",
        "\n",
        "# Derive paper_year: prefer `year`, fallback to year parsed from `date`\n",
        "paper_year = pd.Series(np.nan, index=fa_raw.index, dtype='float64')\n",
        "if 'year' in fa_raw.columns:\n",
        "    paper_year = pd.to_numeric(fa_raw['year'], errors='coerce').astype('float64')\n",
        "if 'date' in fa_raw.columns:\n",
        "    def year_from_date(v: object) -> float | np.float64:\n",
        "        if pd.isna(v):\n",
        "            return np.nan\n",
        "        s = str(v)\n",
        "        m = re.match(r'\\s*(\\d{4})', s)\n",
        "        if m:\n",
        "            try:\n",
        "                return float(m.group(1))\n",
        "            except Exception:\n",
        "                return np.nan\n",
        "        m2 = re.search(r'(\\d{4})', s)\n",
        "        if m2:\n",
        "            try:\n",
        "                return float(m2.group(1))\n",
        "            except Exception:\n",
        "                return np.nan\n",
        "        return np.nan\n",
        "    paper_year = paper_year.fillna(fa_raw['date'].map(year_from_date))\n",
        "fa_raw['paper_year'] = paper_year\n",
        "\n",
        "# Helpers to parse list/dict-like cells robustly (strict ID-only; no name keys)\n",
        "def to_struct(val: object):\n",
        "    if isinstance(val, (list, dict)):\n",
        "        return val\n",
        "    if pd.isna(val):\n",
        "        return None\n",
        "    s = str(val)\n",
        "    if s.strip().lower() in {'', 'none', 'nan', 'null'}:\n",
        "        return None\n",
        "    try:\n",
        "        return json.loads(s)\n",
        "    except Exception:\n",
        "        try:\n",
        "            return ast.literal_eval(s)\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "def extract_first_id(val: object) -> str | None:\n",
        "    obj = to_struct(val)\n",
        "    if obj is None:\n",
        "        return None\n",
        "    if isinstance(obj, list) and obj:\n",
        "        first = obj[0]\n",
        "        if isinstance(first, dict):\n",
        "            fid = first.get('id')\n",
        "            return str(fid) if fid not in (None, '', 'None') else None\n",
        "        # If first element is a string (likely a name), do not use it as key\n",
        "        return None\n",
        "    if isinstance(obj, dict):\n",
        "        fid = obj.get('id')\n",
        "        return str(fid) if fid not in (None, '', 'None') else None\n",
        "    return None\n",
        "\n",
        "# First-author key priority: researchers[0].id -> authors[0].id (no name fallback)\n",
        "fa_key = pd.Series([None]*len(fa_raw), index=fa_raw.index, dtype='object')\n",
        "if 'researchers' in fa_raw.columns:\n",
        "    fa_key = fa_raw['researchers'].map(extract_first_id)\n",
        "if 'authors' in fa_raw.columns:\n",
        "    fallback_ids = fa_raw['authors'].map(extract_first_id)\n",
        "    fa_key = fa_key.fillna(fallback_ids)\n",
        "fa_raw['first_author_key'] = fa_key\n",
        "\n",
        "# Earliest first-author year map (within this dataset)\n",
        "valid_rows = fa_raw.dropna(subset=['first_author_key','paper_year']).copy()\n",
        "valid_rows['paper_year'] = pd.to_numeric(valid_rows['paper_year'], errors='coerce')\n",
        "first_year_map = valid_rows.groupby('first_author_key')['paper_year'].min()\n",
        "\n",
        "# Experience = paper_year - earliest_year, clipped to non-negative\n",
        "delta = fa_raw['paper_year'] - fa_raw['first_author_key'].map(first_year_map)\n",
        "delta = pd.to_numeric(delta, errors='coerce')\n",
        "delta = delta.where(delta.isna() | (delta >= 0), other=0)\n",
        "fa_exp = delta.round().astype('Int16')\n",
        "\n",
        "# Output frames\n",
        "fae = pd.DataFrame({'id': fa_raw['id'], 'first_author_experience': fa_exp})\n",
        "fae_debug = pd.DataFrame({\n",
        "    'id': fa_raw['id'],\n",
        "    'first_author_key': fa_raw['first_author_key'],\n",
        "    'first_author_first_year': fa_raw['first_author_key'].map(first_year_map)\n",
        "})\n",
        "\n",
        "print(fae.head())\n",
        "print(fae['first_author_experience'].value_counts(dropna=False).head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Variables: Controls (document and referencing)\n",
        "\n",
        "We retain common control variables for modeling and descriptive statistics.\n",
        "\n",
        "Columns used: `id`, `document_type`, `type`, `authors_count`, `reference_ids`, `referenced_pubs`.\n",
        "\n",
        "Output columns: `id`, `document_type`, `type`, `authors_count`, `reference_ids`, `referenced_pubs`. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               id       document_type     type authors_count reference_ids referenced_pubs\n",
            "0  pub.1186290333  OTHER_BOOK_CONTENT  chapter             0          None            None\n",
            "1  pub.1186290332                None  chapter             0          None            None\n",
            "2  pub.1186290331      REFERENCE_WORK  chapter             0          None            None\n",
            "3  pub.1186290329                None  chapter             0          None            None\n",
            "4  pub.1186290328                None  chapter             0          None            None\n"
          ]
        }
      ],
      "source": [
        "# Keep control variables\n",
        "ctrl_cols = ['id','document_type','type','authors_count','reference_ids','referenced_pubs']\n",
        "existing_ctrl = [c for c in ctrl_cols if c in pf.schema.names]\n",
        "ctrl = pd.read_parquet(PARQUET_INPUT, engine='pyarrow', columns=existing_ctrl)\n",
        "print(ctrl.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Merge and write output (column order by conceptual blocks)\n",
        "\n",
        "We left-join all module dataframes by `id` and order columns so that variables belonging to the same construct appear together. The final dataset is written as a single Parquet file with Snappy compression.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final shape: (358493, 24)\n",
            "Columns (ordered): ['id', 'invisibility', 'times_cited', 'date', 'research_org_country_names', 'research_org_names', 'research_org_types', 'concepts', 'concepts_scores', 'issn', 'isbn', 'disciplinary', 'best_rank', 'matched_institution', 'matched_token', 'match_type', 'match_score', 'prestige', 'open_access', 'document_type', 'type', 'authors_count', 'reference_ids', 'referenced_pubs']\n",
            "Wrote: /Users/yann.jy/InvisibleResearch/data/processed/dimension_data_for_analysis.parquet\n"
          ]
        }
      ],
      "source": [
        "# Merge by id and order columns\n",
        "from functools import reduce\n",
        "\n",
        "# Ensure required frames exist\n",
        "frames = [inv, geo, top, disc, prest, oa, fae, ctrl]\n",
        "final = reduce(lambda l, r: l.merge(r, on='id', how='left'), frames)\n",
        "\n",
        "# Column ordering by conceptual blocks\n",
        "ordered_cols = (\n",
        "    ['id'] +\n",
        "    ['invisibility','times_cited','date','first_author_experience'] +\n",
        "    ['research_org_country_names','research_org_names','research_org_types'] +\n",
        "    ['concepts','concepts_scores'] +\n",
        "    ['issn','isbn','disciplinary'] +\n",
        "    # Prestige block with matching details\n",
        "    ['best_rank','matched_institution','matched_token','match_type','match_score','prestige'] +\n",
        "    ['open_access'] +\n",
        "    ['document_type','type','authors_count','reference_ids','referenced_pubs']\n",
        ")\n",
        "final_cols = [c for c in ordered_cols if c in final.columns]\n",
        "final = final[final_cols]\n",
        "\n",
        "print('Final shape:', final.shape)\n",
        "print('Columns (ordered):', final.columns.tolist())\n",
        "\n",
        "# Write Parquet (single file)\n",
        "final.to_parquet(OUTPUT_PARQUET, engine='pyarrow', compression='snappy', index=False)\n",
        "print('Wrote:', OUTPUT_PARQUET)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data quality and reliability checks\n",
        "\n",
        "We perform lightweight but informative reliability checks on the final dataset:\n",
        "- Missingness summary (counts and ratios) at column level\n",
        "- Key constraints: `id` uniqueness and row-count parity against the base table\n",
        "- Domain checks: `invisibility` in {0,1}; non-negativity for `times_cited` and `authors_count`\n",
        "- Format checks: ISSN/ISBN token validity (normalized tokens of length 8 with digits/X)\n",
        "- Distribution snapshots: value counts or quantiles for selected variables\n",
        "- Risk indicators: normalized duplicate DOI (if a DOI column exists upstream), rows missing both `issn` and `isbn`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final shape (reloaded): (358493, 24)\n",
            "\n",
            "Variable columns missingness (NA + empty strings), sorted by na_ratio (final output columns only):\n",
            "                            na_count  na_ratio\n",
            "isbn                          274561    0.7659\n",
            "research_org_types            184569    0.5148\n",
            "research_org_country_names    176122    0.4913\n",
            "reference_ids                 169944    0.4741\n",
            "referenced_pubs               169945    0.4741\n",
            "research_org_names            164923    0.4600\n",
            "issn                           78802    0.2198\n",
            "document_type                  75065    0.2094\n",
            "concepts                       25829    0.0720\n",
            "concepts_scores                25829    0.0720\n",
            "times_cited                        0    0.0000\n",
            "disciplinary                       0    0.0000\n",
            "prestige                           0    0.0000\n",
            "open_access                        0    0.0000\n",
            "type                               0    0.0000\n",
            "authors_count                      0    0.0000\n",
            "date                               0    0.0000\n",
            "invisibility                       2    0.0000\n",
            "\n",
            "ID uniqueness check:\n",
            "Total ids: 358493 Distinct ids: 358493\n",
            "\n",
            "Invisibility value counts:\n",
            "invisibility\n",
            "0       186551\n",
            "1       171940\n",
            "<NA>         2\n",
            "Name: count, dtype: Int64\n",
            "\n",
            "Times cited quantiles:\n",
            "count    358491.000000\n",
            "mean          9.033242\n",
            "std          52.154471\n",
            "min           0.000000\n",
            "50%           1.000000\n",
            "90%          19.000000\n",
            "99%         130.000000\n",
            "max        9939.000000\n",
            "Name: times_cited, dtype: float64\n",
            "Negative times_cited count: 0\n",
            "\n",
            "Authors count quantiles:\n",
            "count    358491.000000\n",
            "mean          1.875863\n",
            "std           1.696753\n",
            "min           0.000000\n",
            "50%           1.000000\n",
            "90%           4.000000\n",
            "99%           7.000000\n",
            "max         241.000000\n",
            "Name: authors_count, dtype: float64\n",
            "Negative authors_count count: 0\n",
            "\n",
            "Rows missing both issn and isbn (NA + empty strings): 10974\n",
            "Share of rows with a token that looks like ISSN: 0.0477\n",
            "\n",
            "Duplicate DOI (normalized) at base level: 1028\n"
          ]
        }
      ],
      "source": [
        "# Load final and run checks\n",
        "final_df = pd.read_parquet(OUTPUT_PARQUET, engine='pyarrow')\n",
        "print('Final shape (reloaded):', final_df.shape)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Expanded missingness for variable-related columns ONLY (treat empty string '' as NA)\n",
        "var_cols = [\n",
        "    # Invisibility block\n",
        "    'invisibility','times_cited','date','first_author_experience',\n",
        "    # Geographic/Institutional (include names/types as requested)\n",
        "    'research_org_country_names','research_org_names','research_org_types',\n",
        "    # Topical\n",
        "    'concepts','concepts_scores',\n",
        "    # Disciplinary\n",
        "    'issn','isbn','disciplinary',\n",
        "    # Prestige\n",
        "    'prestige',\n",
        "    # OA\n",
        "    'open_access',\n",
        "    # Controls\n",
        "    'document_type','type','authors_count','reference_ids','referenced_pubs'\n",
        "]\n",
        "var_cols = [c for c in var_cols if c in final_df.columns]\n",
        "\n",
        "def is_blank(s: pd.Series) -> pd.Series:\n",
        "    return s.isna() | s.astype(str).str.strip().eq('')\n",
        "\n",
        "var_na_counts = {c: int(is_blank(final_df[c]).sum()) for c in var_cols}\n",
        "var_na_ratio = {c: round(var_na_counts[c] / len(final_df), 4) for c in var_cols}\n",
        "var_missing = (\n",
        "    pd.DataFrame({'na_count': pd.Series(var_na_counts), 'na_ratio': pd.Series(var_na_ratio)})\n",
        "      .sort_values('na_ratio', ascending=False)\n",
        ")\n",
        "print('\\nVariable columns missingness (NA + empty strings), sorted by na_ratio (final output columns only):')\n",
        "print(var_missing)\n",
        "\n",
        "# Key constraints\n",
        "print('\\nID uniqueness check:')\n",
        "print('Total ids:', final_df['id'].size, 'Distinct ids:', final_df['id'].nunique())\n",
        "\n",
        "# Domain checks\n",
        "if 'invisibility' in final_df.columns:\n",
        "    print('\\nInvisibility value counts:')\n",
        "    print(final_df['invisibility'].value_counts(dropna=False))\n",
        "\n",
        "if 'times_cited' in final_df.columns:\n",
        "    # Coerce to numeric for robust comparison and quantiles\n",
        "    tc = pd.to_numeric(final_df['times_cited'], errors='coerce')\n",
        "    print('\\nTimes cited quantiles:')\n",
        "    print(tc.describe(percentiles=[0.5,0.9,0.99]))\n",
        "    neg_tc = (tc.dropna() < 0).sum()\n",
        "    print('Negative times_cited count:', int(neg_tc))\n",
        "\n",
        "if 'authors_count' in final_df.columns:\n",
        "    ac = pd.to_numeric(final_df['authors_count'], errors='coerce')\n",
        "    print('\\nAuthors count quantiles:')\n",
        "    print(ac.describe(percentiles=[0.5,0.9,0.99]))\n",
        "    neg_ac = (ac.dropna() < 0).sum()\n",
        "    print('Negative authors_count count:', int(neg_ac))\n",
        "\n",
        "# Format checks for identifiers\n",
        "import re\n",
        "TOK = re.compile(r'[0-9A-Za-z]+')\n",
        "\n",
        "def norm_tokens(val: object) -> list[str]:\n",
        "    if pd.isna(val):\n",
        "        return []\n",
        "    tokens = TOK.findall(str(val))\n",
        "    tokens = [re.sub(r'[^0-9X]', '', t.upper()) for t in tokens]\n",
        "    return [t for t in tokens if t]\n",
        "\n",
        "if ('issn' in final_df.columns) or ('isbn' in final_df.columns):\n",
        "    def valid_issn_like(x: object) -> bool:\n",
        "        toks = norm_tokens(x)\n",
        "        # Accept any token with length 8\n",
        "        return any(len(t) == 8 for t in toks)\n",
        "    both_missing = 0\n",
        "    if ('issn' in final_df.columns) and ('isbn' in final_df.columns):\n",
        "        both_missing = (is_blank(final_df['issn']) & is_blank(final_df['isbn'])).sum()\n",
        "    print('\\nRows missing both issn and isbn (NA + empty strings):', int(both_missing))\n",
        "    if 'issn' in final_df.columns:\n",
        "        print('Share of rows with a token that looks like ISSN:', round(final_df['issn'].map(valid_issn_like).mean(), 4))\n",
        "\n",
        "# Risk indicators (base-level DOI duplicates if DOI exists upstream)\n",
        "if 'doi' in pf.schema.names:\n",
        "    doi_base = pd.read_parquet(PARQUET_INPUT, engine='pyarrow', columns=['id', 'doi'])\n",
        "    doi_base['doi_norm'] = (\n",
        "        doi_base['doi'].astype(str)\n",
        "        .str.lower()\n",
        "        .str.replace(r'^https?://(dx\\.)?doi\\.org/', '', regex=True)\n",
        "        .str.replace(r'\\s+', '', regex=True)\n",
        "    )\n",
        "    dup = doi_base[\n",
        "        doi_base['doi_norm'].ne('') & doi_base['doi_norm'].notna()\n",
        "    ].duplicated('doi_norm', keep=False).sum()\n",
        "    print('\\nDuplicate DOI (normalized) at base level:', int(dup))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# QA extension: first_author_experience checks\n",
        "try:\n",
        "    _ = final_df  # reuse if available\n",
        "except NameError:\n",
        "    final_df = pd.read_parquet(OUTPUT_PARQUET, engine='pyarrow')\n",
        "\n",
        "if 'first_author_experience' in final_df.columns:\n",
        "    fae_num = pd.to_numeric(final_df['first_author_experience'], errors='coerce')\n",
        "    print('\\nfirst_author_experience missing count:', int(fae_num.isna().sum()))\n",
        "    print('first_author_experience value counts (top 10 incl. NA):')\n",
        "    print(fae_num.value_counts(dropna=False).head(10))\n",
        "    print('\\nfirst_author_experience quantiles:')\n",
        "    print(fae_num.describe(percentiles=[0.5, 0.9, 0.99]))\n",
        "    neg_count = int((fae_num.dropna() < 0).sum())\n",
        "    print('Negative first_author_experience count:', neg_count)\n",
        "else:\n",
        "    print('first_author_experience not found in final output columns')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
