{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dimension-derived Variable Construction (English-only)\n",
        "\n",
        "This notebook derives analysis-ready variables from the unified Dimensions dataset and writes a single Parquet for downstream analyses. The workflow adheres to academic reporting best practices: each variable is introduced by a short rationale (markdown) followed by a reproducible code cell. No intermediate files are written; only the final dataset is saved.\n",
        "\n",
        "- Source dataset: `/Users/yann.jy/InvisibleResearch/data/processed/dimension_merged.parquet`\n",
        "- External reference (for disciplinary core journal matching): `/Users/yann.jy/InvisibleResearch/data/processed/scimagojr_communication_journal_1999_2024.csv`\n",
        "- Output dataset: `/Users/yann.jy/InvisibleResearch/data/processed/dimension_data_for_analysis.parquet`\n",
        "\n",
        "Conventions:\n",
        "- All module outputs include `id` for reliable merges.\n",
        "- Column blocks are ordered by conceptual variable groups so related columns appear together in the final file.\n",
        "- Time-window logic for invisibility is intentionally not applied here; analyses may add temporal filters later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input exists: True\n",
            "SJR CSV exists: True\n",
            "Output path: /Users/yann.jy/InvisibleResearch/data/processed/dimension_data_for_analysis.parquet\n"
          ]
        }
      ],
      "source": [
        "# Setup and paths\n",
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "# Paths (absolute as requested)\n",
        "PROJECT_ROOT = Path('/Users/yann.jy/InvisibleResearch')\n",
        "PARQUET_INPUT = PROJECT_ROOT / 'data/processed/dimension_merged.parquet'\n",
        "SJR_CSV = PROJECT_ROOT / 'data/processed/scimagojr_communication_journal_1999_2024.csv'\n",
        "OUTPUT_PARQUET = PROJECT_ROOT / 'data/processed/dimension_data_for_analysis.parquet'\n",
        "\n",
        "pd.set_option('display.max_colwidth', 160)\n",
        "pd.set_option('display.width', 160)\n",
        "\n",
        "print('Input exists:', PARQUET_INPUT.exists())\n",
        "print('SJR CSV exists:', SJR_CSV.exists())\n",
        "print('Output path:', OUTPUT_PARQUET)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Schema preview\n",
        "\n",
        "We first inspect the Parquet schema and a small sample to confirm column availability and obtain a quick sense of the data. No full-table prints are performed to avoid large outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of columns: 76\n",
            "Columns:\n",
            "['abstract', 'acknowledgements', 'altmetric', 'altmetric_id', 'arxiv_id', 'authors', 'authors_count', 'book_doi', 'book_series_title', 'book_title', 'category_bra', 'category_for', 'category_for_2020', 'category_hra', 'category_hrcs_hc', 'category_hrcs_rac', 'category_icrp_cso', 'category_icrp_ct', 'category_rcdc', 'category_sdg', 'category_uoa', 'clinical_trial_ids', 'concepts', 'concepts_scores', 'date', 'date_inserted', 'date_online', 'date_print', 'dimensions_url', 'document_type', 'doi', 'editors', 'field_citation_ratio', 'funder_countries', 'funders', 'funding_section', 'id', 'isbn', 'issn', 'issue', 'journal.id', 'journal.title', 'journal_lists', 'journal_title_raw', 'linkout', 'mesh_terms', 'open_access', 'pages', 'pmcid', 'pmid', 'proceedings_title', 'publisher', 'recent_citations', 'reference_ids', 'referenced_pubs', 'relative_citation_ratio', 'research_org_cities', 'research_org_countries', 'research_org_country_names', 'research_org_names', 'research_org_state_codes', 'research_org_state_names', 'research_org_types', 'research_orgs', 'researchers', 'resulting_publication_doi', 'score', 'source_title.id', 'source_title.title', 'subtitles', 'supporting_grant_ids', 'times_cited', 'title', 'type', 'volume', 'year']\n",
            "Total rows (metadata): 358493\n",
            "\n",
            "Head (subset of columns):\n",
            "                                                                                                                                                          abstract  \\\n",
            "0  <p>Coverage of the Clinton-Lewinsky saga followed in a long trail of media exposures of the more personal details of the lives of public figures. Many comme...   \n",
            "1  <p>Coverage of the Clinton-Lewinsky saga followed in a long trail of media exposures of the more personal details of the lives of public figures. Many comme...   \n",
            "2  <p>Coverage of the Clinton-Lewinsky saga followed in a long trail of media exposures of the more personal details of the lives of public figures. Many comme...   \n",
            "3  <p>Coverage of the Clinton-Lewinsky saga followed in a long trail of media exposures of the more personal details of the lives of public figures. Many comme...   \n",
            "4  <p>Coverage of the Clinton-Lewinsky saga followed in a long trail of media exposures of the more personal details of the lives of public figures. Many comme...   \n",
            "\n",
            "  acknowledgements altmetric altmetric_id arxiv_id authors authors_count               book_doi book_series_title     book_title category_bra  \\\n",
            "0             None      None          0.0     None    None             0  10.5771/9781461643852              None  Tabloid Tales         None   \n",
            "1             None      None          0.0     None    None             0  10.5771/9781461643852              None  Tabloid Tales         None   \n",
            "2             None      None          0.0     None    None             0  10.5771/9781461643852              None  Tabloid Tales         None   \n",
            "3             None      None          0.0     None    None             0  10.5771/9781461643852              None  Tabloid Tales         None   \n",
            "4             None      None          0.0     None    None             0  10.5771/9781461643852              None  Tabloid Tales         None   \n",
            "\n",
            "                                                                                                                                                      category_for  \n",
            "0  [{'id': '80007', 'name': '36 Creative Arts and Writing'}, {'id': '80194', 'name': '4701 Communication and Media Studies'}, {'id': '80018', 'name': '47 Langu...  \n",
            "1  [{'id': '80007', 'name': '36 Creative Arts and Writing'}, {'id': '80194', 'name': '4701 Communication and Media Studies'}, {'id': '80018', 'name': '47 Langu...  \n",
            "2  [{'id': '80007', 'name': '36 Creative Arts and Writing'}, {'id': '80194', 'name': '4701 Communication and Media Studies'}, {'id': '80018', 'name': '47 Langu...  \n",
            "3  [{'id': '80007', 'name': '36 Creative Arts and Writing'}, {'id': '80194', 'name': '4701 Communication and Media Studies'}, {'id': '80018', 'name': '47 Langu...  \n",
            "4  [{'id': '80007', 'name': '36 Creative Arts and Writing'}, {'id': '80194', 'name': '4701 Communication and Media Studies'}, {'id': '80018', 'name': '47 Langu...  \n"
          ]
        }
      ],
      "source": [
        "# Read schema (column names) and show a small preview\n",
        "pf = pq.ParquetFile(PARQUET_INPUT)\n",
        "print('Number of columns:', len(pf.schema.names))\n",
        "print('Columns:')\n",
        "print(pf.schema.names)\n",
        "\n",
        "# Lightweight row count and a tiny data preview\n",
        "num_rows = pf.metadata.num_rows\n",
        "print('Total rows (metadata):', num_rows)\n",
        "\n",
        "# Sample a few rows without loading entire file (avoid unsupported filters arg)\n",
        "base_head = pd.read_parquet(PARQUET_INPUT, engine='pyarrow', columns=pf.schema.names[:12]).head(5)\n",
        "print('\\nHead (subset of columns):')\n",
        "print(base_head)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Variable: Invisibility (times_cited only)\n",
        "\n",
        "Definition: A record is labeled as invisible if it has received zero citations, i.e., `invisibility = 1` when `times_cited == 0`; otherwise `0`. We retain `date` for analytical context but do not use it in the rule here. All outputs include `id` for merging.\n",
        "\n",
        "Columns used: `id`, `times_cited`, `date`.\n",
        "\n",
        "Output columns: `id`, `invisibility`, `times_cited`, `date`. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               id  invisibility times_cited        date\n",
            "0  pub.1186290333             1           0  2000-01-01\n",
            "1  pub.1186290332             1           0  2000-01-01\n",
            "2  pub.1186290331             1           0  2000-01-01\n",
            "3  pub.1186290329             1           0  2000-01-01\n",
            "4  pub.1186290328             1           0  2000-01-01\n",
            "invisibility\n",
            "0       186551\n",
            "1       171940\n",
            "<NA>         2\n",
            "Name: invisibility_counts, dtype: Int64\n"
          ]
        }
      ],
      "source": [
        "# Build invisibility (robust to non-numeric times_cited)\n",
        "base = pd.read_parquet(PARQUET_INPUT, engine='pyarrow', columns=['id','times_cited','date'])\n",
        "inv = base[['id','times_cited','date']].copy()\n",
        "# Convert to numeric, coercing invalid tokens (e.g., stray dict fragments) to NaN\n",
        "_tc = pd.to_numeric(inv['times_cited'], errors='coerce')\n",
        "# Initialize as NA; then fill where numeric is available\n",
        "inv['invisibility'] = pd.Series(pd.NA, index=inv.index, dtype='Int8')\n",
        "mask_num = _tc.notna()\n",
        "inv.loc[mask_num, 'invisibility'] = (_tc.loc[mask_num] == 0).astype('int8')\n",
        "inv['invisibility'] = inv['invisibility'].astype('Int8')\n",
        "# Order output columns as specified\n",
        "inv = inv[['id','invisibility','times_cited','date']]\n",
        "print(inv.head())\n",
        "print(inv['invisibility'].value_counts(dropna=False).rename('invisibility_counts'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Variable: Geographic (institutional location)\n",
        "\n",
        "We retain institutional location fields to support geographic analyses. No transformation is applied here; the values may contain multiple institutions and countries per record.\n",
        "\n",
        "Columns used: `id`, `research_org_country_names`, `research_org_names`, `research_org_types`.\n",
        "\n",
        "Output columns: `id`, `research_org_country_names`, `research_org_names`, `research_org_types`. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               id research_org_country_names research_org_names research_org_types\n",
            "0  pub.1186290333                       None               None               None\n",
            "1  pub.1186290332                       None               None               None\n",
            "2  pub.1186290331                       None               None               None\n",
            "3  pub.1186290329                       None               None               None\n",
            "4  pub.1186290328                       None               None               None\n"
          ]
        }
      ],
      "source": [
        "# Keep geographic fields\n",
        "geo_cols = ['id','research_org_country_names','research_org_names','research_org_types']\n",
        "existing_geo = [c for c in geo_cols if c in pf.schema.names]\n",
        "geo = pd.read_parquet(PARQUET_INPUT, engine='pyarrow', columns=existing_geo)\n",
        "print(geo.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Variable: Topical (concepts)\n",
        "\n",
        "We retain topic-related fields to enable later construction of mainstream-topic shares or binary mainstream indicators. No transformation is performed here.\n",
        "\n",
        "Columns used: `id`, `concepts`, `concepts_scores`.\n",
        "\n",
        "Output columns: `id`, `concepts`, `concepts_scores`. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               id  \\\n",
            "0  pub.1186290333   \n",
            "1  pub.1186290332   \n",
            "2  pub.1186290331   \n",
            "3  pub.1186290329   \n",
            "4  pub.1186290328   \n",
            "\n",
            "                                                                                                                                                          concepts  \\\n",
            "0  ['mass media', 'news habits', 'mass communication', 'public figures', 'media exposure', 'scholarly studies', 'private life', 'tabloids', 'news', 'U.S. probl...   \n",
            "1  ['mass media', 'news habits', 'mass communication', 'public figures', 'media exposure', 'tabloids', 'scholarly studies', 'private life', 'U.S. problems', 'n...   \n",
            "2  ['mass media', 'news habits', 'mass communication', 'public figures', 'media exposure', 'scholarly studies', 'private life', 'U.S. problems', 'tabloids', 'n...   \n",
            "3  ['mass media', 'tabloid journalism', 'news habits', 'mass communication', 'public figures', 'media exposure', 'scholarly studies', 'tabloids', 'private life...   \n",
            "4  ['mass media', 'tabloid journalism', 'news habits', 'mass communication', 'public figures', 'media exposure', 'scholarly studies', 'tabloids', 'private life...   \n",
            "\n",
            "                                                                                                                                                   concepts_scores  \n",
            "0  [{'concept': 'mass media', 'relevance': 0.798}, {'concept': 'news habits', 'relevance': 0.701}, {'concept': 'mass communication', 'relevance': 0.695}, {'con...  \n",
            "1  [{'concept': 'mass media', 'relevance': 0.798}, {'concept': 'news habits', 'relevance': 0.701}, {'concept': 'mass communication', 'relevance': 0.695}, {'con...  \n",
            "2  [{'concept': 'mass media', 'relevance': 0.798}, {'concept': 'news habits', 'relevance': 0.701}, {'concept': 'mass communication', 'relevance': 0.695}, {'con...  \n",
            "3  [{'concept': 'mass media', 'relevance': 0.798}, {'concept': 'tabloid journalism', 'relevance': 0.701}, {'concept': 'news habits', 'relevance': 0.701}, {'con...  \n",
            "4  [{'concept': 'mass media', 'relevance': 0.798}, {'concept': 'tabloid journalism', 'relevance': 0.701}, {'concept': 'news habits', 'relevance': 0.701}, {'con...  \n"
          ]
        }
      ],
      "source": [
        "# Keep topical fields\n",
        "top_cols = ['id','concepts','concepts_scores']\n",
        "existing_top = [c for c in top_cols if c in pf.schema.names]\n",
        "top = pd.read_parquet(PARQUET_INPUT, engine='pyarrow', columns=existing_top)\n",
        "print(top.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Variable: Disciplinary (core journal hit via SJR)\n",
        "\n",
        "We construct a binary indicator for whether the record's journal identifiers intersect the SJR communication journals ISSN list. The variable equals 1 if any standardized token from `issn` or `isbn` matches any SJR `Issn`; otherwise 0.\n",
        "\n",
        "Columns used: `id`, `issn`, `isbn` (from Dimensions) and `Issn` (from SJR CSV).\n",
        "\n",
        "Output columns: `id`, `issn`, `isbn`, `disciplinary`.\n",
        "\n",
        "Normalization policy:\n",
        "- Uppercase tokens; drop hyphens/whitespace; retain only digits and `X`.\n",
        "- Support multi-valued identifiers split by common non-alphanumerics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               id  issn               isbn  disciplinary\n",
            "0  pub.1186290333  None  ['9781461643852']             0\n",
            "1  pub.1186290332  None  ['9781461643852']             0\n",
            "2  pub.1186290331  None  ['9781461643852']             0\n",
            "3  pub.1186290329  None  ['9781461643852']             0\n",
            "4  pub.1186290328  None  ['9781461643852']             0\n",
            "disciplinary\n",
            "0    356157\n",
            "1      2336\n",
            "Name: disciplinary_counts, dtype: Int64\n"
          ]
        }
      ],
      "source": [
        "# Build disciplinary via SJR ISSN match (robust to multi-valued fields)\n",
        "import re\n",
        "\n",
        "# Base identifiers\n",
        "disc_cols = ['id','issn','isbn']\n",
        "existing_disc = [c for c in disc_cols if c in pf.schema.names]\n",
        "disc = pd.read_parquet(PARQUET_INPUT, engine='pyarrow', columns=existing_disc)\n",
        "\n",
        "# --- Utilities ---\n",
        "TOK = re.compile(r'[0-9A-Za-z]+')\n",
        "\n",
        "def issn_tokens(val: object) -> set[str]:\n",
        "    \"\"\"Extract standardized ISSN-like tokens (length==8, digits/X) from any string-ish value.\n",
        "    Handles multi-valued cases separated by commas/semicolons/spaces/brackets/quotes/hyphens.\n",
        "    \"\"\"\n",
        "    if pd.isna(val):\n",
        "        return set()\n",
        "    # split into alnum chunks, then strip to [0-9X], uppercase\n",
        "    raw = TOK.findall(str(val))\n",
        "    cleaned = [re.sub(r'[^0-9X]', '', t.upper()) for t in raw]\n",
        "    # keep only 8-char tokens (ISSN normalized without hyphen)\n",
        "    return {t for t in cleaned if len(t) == 8}\n",
        "\n",
        "# --- Build SJR ISSN set (SJR side may also contain comma-separated values) ---\n",
        "sjr = pd.read_csv(SJR_CSV, dtype=str, usecols=['Issn'])\n",
        "sjr_issn_set: set[str] = set()\n",
        "for s in sjr['Issn'].astype(str):\n",
        "    sjr_issn_set.update(issn_tokens(s))\n",
        "\n",
        "# --- Match against SJR set ---\n",
        "# Prefer explicit column checks for clarity\n",
        "has_issn = 'issn' in disc.columns\n",
        "has_isbn = 'isbn' in disc.columns\n",
        "\n",
        "issn_hit = pd.Series(False, index=disc.index)\n",
        "if has_issn:\n",
        "    issn_hit = disc['issn'].map(issn_tokens).apply(lambda s: any(t in sjr_issn_set for t in s))\n",
        "\n",
        "isbn_hit = pd.Series(False, index=disc.index)\n",
        "if has_isbn:\n",
        "    # ISBN tokens typically not length 8; this filter ensures only ISSN-like tokens can match\n",
        "    isbn_hit = disc['isbn'].map(issn_tokens).apply(lambda s: any(t in sjr_issn_set for t in s))\n",
        "\n",
        "disc['disciplinary'] = (issn_hit | isbn_hit).astype('Int8')\n",
        "print(disc.head())\n",
        "print(disc['disciplinary'].value_counts(dropna=False).rename('disciplinary_counts'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Variable: Prestige (institutional ranking category)\n",
        "\n",
        "We derive a prestige category by fuzzy-matching `research_org_names` against SCImago Institution Rankings (communication), then mapping rank to bins:\n",
        "- Elite: top 100\n",
        "- High: 101–500\n",
        "- Medium: 501–1000\n",
        "- Low: Unranked or no confident match\n",
        "\n",
        "Inputs:\n",
        "- `research_org_names` (may contain multiple institutions per record)\n",
        "- Rankings CSV: `data/processed/scimagoir_2025_Overall Rank_Communication.csv` (expects columns like `institution` and an overall-rank column)\n",
        "\n",
        "Output columns: `id`, `prestige`. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Cannot locate institution name column in SCImagoIR CSV",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name_col \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mCannot locate institution name column in SCImagoIR CSV\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     29\u001b[39m rank_col = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cand \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33moverall_rank\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33moverall rank\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mOverall Rank\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mRank\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mrank\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mOverallRank\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33moverallRank\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mrank_overall\u001b[39m\u001b[33m'\u001b[39m]:\n",
            "\u001b[31mRuntimeError\u001b[39m: Cannot locate institution name column in SCImagoIR CSV"
          ]
        }
      ],
      "source": [
        "# Build prestige from SCImagoIR 2025 (Communication) by fuzzy-matching\n",
        "import re\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "rank_csv = PROJECT_ROOT / 'data/processed/scimagoir_2025_Overall Rank_Communication.csv'\n",
        "# Try robust CSV reading: auto-detect delimiter, handle quoted fields and bad lines\n",
        "try:\n",
        "    # First try semicolon (observed in file header)\n",
        "    rank_df = pd.read_csv(rank_csv, dtype=str, sep=';', engine='python')\n",
        "except Exception:\n",
        "    # Fallbacks: auto and common delimiters with skipping bad lines\n",
        "    for sep in [None, ',', '\\t', '|']:\n",
        "        try:\n",
        "            rank_df = pd.read_csv(rank_csv, dtype=str, sep=sep, engine='python', on_bad_lines='skip')\n",
        "            break\n",
        "        except Exception:\n",
        "            rank_df = None\n",
        "    if rank_df is None:\n",
        "        raise\n",
        "\n",
        "# Heuristics to detect rank and institution columns (support semicolon CSVs)\n",
        "# Normalize headers first for robust matching\n",
        "rank_df.columns = [c.strip() for c in rank_df.columns]\n",
        "\n",
        "name_col = None\n",
        "for cand in ['institution','Institution','Global Institution','Global institution','Global  Institution']:\n",
        "    if cand in rank_df.columns:\n",
        "        name_col = cand\n",
        "        break\n",
        "if name_col is None:\n",
        "    # Try case-insensitive match\n",
        "    lower_map = {c.lower(): c for c in rank_df.columns}\n",
        "    for key in ['institution','global rank institution','global institution','name']:\n",
        "        if key in lower_map:\n",
        "            name_col = lower_map[key]\n",
        "            break\n",
        "if name_col is None:\n",
        "    # As last resort, if a column contains many alphabetic strings and not numeric-only, pick it\n",
        "    texty = [c for c in rank_df.columns if rank_df[c].astype(str).str.contains(r\"[A-Za-z]\", regex=True).mean() > 0.8]\n",
        "    if texty:\n",
        "        name_col = texty[0]\n",
        "if name_col is None:\n",
        "    raise RuntimeError(f'Cannot locate institution name column in SCImagoIR CSV. Columns: {list(rank_df.columns)}')\n",
        "\n",
        "rank_col = None\n",
        "for cand in ['overall_rank','overall rank','Overall Rank','Rank','rank','Global Rank','Global rank','GlobalRank']:\n",
        "    if cand in rank_df.columns:\n",
        "        rank_col = cand\n",
        "        break\n",
        "if rank_col is None:\n",
        "    # Case-insensitive fallback\n",
        "    lower_map = {c.lower(): c for c in rank_df.columns}\n",
        "    for key in ['global rank','rank','overall rank']:\n",
        "        if key in lower_map:\n",
        "            rank_col = lower_map[key]\n",
        "            break\n",
        "if rank_col is None:\n",
        "    # Numeric-looking dominant column\n",
        "    numeric_like = []\n",
        "    for c in rank_df.columns:\n",
        "        try:\n",
        "            vals = pd.to_numeric(rank_df[c], errors='coerce')\n",
        "            if vals.notna().mean() > 0.8:\n",
        "                numeric_like.append((c, vals))\n",
        "        except Exception:\n",
        "            pass\n",
        "    if numeric_like:\n",
        "        rank_col = numeric_like[0][0]\n",
        "if rank_col is None:\n",
        "    raise RuntimeError(f'Cannot locate rank column in SCImagoIR CSV. Columns: {list(rank_df.columns)}')\n",
        "\n",
        "# Normalize institution names and numeric rank\n",
        "def norm_text(x: object) -> str:\n",
        "    s = str(x) if pd.notna(x) else ''\n",
        "    s = s.lower().strip()\n",
        "    s = re.sub(r\"[\\-–—'’`\\\"]\", ' ', s)\n",
        "    s = re.sub(r\"[^a-z0-9&\\s]\", ' ', s)\n",
        "    s = re.sub(r\"\\s+\", ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "rank_df = rank_df[[name_col, rank_col]].copy()\n",
        "rank_df[name_col] = rank_df[name_col].map(norm_text)\n",
        "rank_df[rank_col] = pd.to_numeric(rank_df[rank_col], errors='coerce')\n",
        "rank_df = rank_df.dropna(subset=[name_col])\n",
        "\n",
        "# Build a fast lookup: exact normalized name → best rank\n",
        "rank_map = (\n",
        "    rank_df.dropna(subset=[rank_col])\n",
        "           .sort_values(rank_col)\n",
        "           .drop_duplicates(subset=[name_col], keep='first')\n",
        "           .set_index(name_col)[rank_col]\n",
        "           .to_dict()\n",
        ")\n",
        "rank_names = list(rank_map.keys())\n",
        "\n",
        "# Helper: best fuzzy match per input string using SequenceMatcher ratio\n",
        "def best_fuzzy_rank(q: str, threshold: float = 0.86) -> float | None:\n",
        "    qn = norm_text(q)\n",
        "    if not qn:\n",
        "        return None\n",
        "    # Exact hit first\n",
        "    if qn in rank_map:\n",
        "        return float(rank_map[qn])\n",
        "    # Fallback: fuzzy similarity\n",
        "    best_rank = None\n",
        "    best_score = 0.0\n",
        "    for cand in rank_names:\n",
        "        score = SequenceMatcher(None, qn, cand).ratio()\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_rank = float(rank_map[cand])\n",
        "    return best_rank if best_score >= threshold else None\n",
        "\n",
        "# Read research_org_names from the geographic frame (may contain multi-values)\n",
        "geo_names = pd.read_parquet(PARQUET_INPUT, engine='pyarrow', columns=['id','research_org_names'])\n",
        "\n",
        "def split_names(val: object) -> list[str]:\n",
        "    if pd.isna(val):\n",
        "        return []\n",
        "    # Split by commas/semicolons/slashes/pipes and collapse bracket/quote noise\n",
        "    s = str(val)\n",
        "    # Replace brackets/quotes with space\n",
        "    s = re.sub(r\"[\\[\\]\\(\\)\\{\\}\\'\\\"]\", ' ', s)\n",
        "    # Then split on common delimiters or 2+ spaces\n",
        "    tokens = re.split(r\"[;,/\\|]|\\s{2,}\", s)\n",
        "    # Drop empties and very short tokens\n",
        "    return [t.strip() for t in tokens if t and t.strip()]\n",
        "\n",
        "# For each row, compute best (lowest) rank among matched institutions\n",
        "records = []\n",
        "for rid, names in geo_names[['id','research_org_names']].itertuples(index=False):\n",
        "    ranks = []\n",
        "    for n in split_names(names):\n",
        "        r = best_fuzzy_rank(n)\n",
        "        if r is not None:\n",
        "            ranks.append(r)\n",
        "    best = min(ranks) if ranks else None\n",
        "    records.append((rid, best))\n",
        "\n",
        "prest_df = pd.DataFrame(records, columns=['id','best_rank'])\n",
        "\n",
        "# Map to bins\n",
        "def to_prestige(r: float | None) -> str:\n",
        "    if r is None or np.isnan(r):\n",
        "        return 'Low'  # Unranked\n",
        "    r = float(r)\n",
        "    if r <= 100:\n",
        "        return 'Elite'\n",
        "    if r <= 500:\n",
        "        return 'High'\n",
        "    if r <= 1000:\n",
        "        return 'Medium'\n",
        "    return 'Low'\n",
        "\n",
        "prest_df['prestige'] = prest_df['best_rank'].map(to_prestige)\n",
        "prest = prest_df[['id','prestige']]\n",
        "print(prest.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Variable: Open Access status\n",
        "\n",
        "We retain the open access indicator as-is for subsequent stratified analyses.\n",
        "\n",
        "Columns used: `id`, `open_access`.\n",
        "\n",
        "Output columns: `id`, `open_access`. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Keep OA field\n",
        "oa_cols = ['id','open_access']\n",
        "existing_oa = [c for c in oa_cols if c in pf.schema.names]\n",
        "oa = pd.read_parquet(PARQUET_INPUT, engine='pyarrow', columns=existing_oa)\n",
        "print(oa['open_access'].value_counts(dropna=False).head())\n",
        "print(oa.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Variables: Controls (document and referencing)\n",
        "\n",
        "We retain common control variables for modeling and descriptive statistics.\n",
        "\n",
        "Columns used: `id`, `document_type`, `type`, `authors_count`, `reference_ids`, `referenced_pubs`.\n",
        "\n",
        "Output columns: `id`, `document_type`, `type`, `authors_count`, `reference_ids`, `referenced_pubs`. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Keep control variables\n",
        "ctrl_cols = ['id','document_type','type','authors_count','reference_ids','referenced_pubs']\n",
        "existing_ctrl = [c for c in ctrl_cols if c in pf.schema.names]\n",
        "ctrl = pd.read_parquet(PARQUET_INPUT, engine='pyarrow', columns=existing_ctrl)\n",
        "print(ctrl.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Merge and write output (column order by conceptual blocks)\n",
        "\n",
        "We left-join all module dataframes by `id` and order columns so that variables belonging to the same construct appear together. The final dataset is written as a single Parquet file with Snappy compression.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge by id and order columns\n",
        "from functools import reduce\n",
        "\n",
        "# Ensure required frames exist\n",
        "frames = [inv, geo, top, disc, prest, oa, ctrl]\n",
        "final = reduce(lambda l, r: l.merge(r, on='id', how='left'), frames)\n",
        "\n",
        "# Column ordering by conceptual blocks\n",
        "ordered_cols = (\n",
        "    ['id'] +\n",
        "    ['invisibility','times_cited','date'] +\n",
        "    ['research_org_country_names','research_org_names','research_org_types'] +\n",
        "    ['concepts','concepts_scores'] +\n",
        "    ['issn','isbn','disciplinary'] +\n",
        "    ['open_access'] +\n",
        "    ['document_type','type','authors_count','reference_ids','referenced_pubs']\n",
        ")\n",
        "final_cols = [c for c in ordered_cols if c in final.columns]\n",
        "final = final[final_cols]\n",
        "\n",
        "print('Final shape:', final.shape)\n",
        "print('Columns (ordered):', final.columns.tolist())\n",
        "\n",
        "# Write Parquet (single file)\n",
        "final.to_parquet(OUTPUT_PARQUET, engine='pyarrow', compression='snappy', index=False)\n",
        "print('Wrote:', OUTPUT_PARQUET)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data quality and reliability checks\n",
        "\n",
        "We perform lightweight but informative reliability checks on the final dataset:\n",
        "- Missingness summary (counts and ratios) at column level\n",
        "- Key constraints: `id` uniqueness and row-count parity against the base table\n",
        "- Domain checks: `invisibility` in {0,1}; non-negativity for `times_cited` and `authors_count`\n",
        "- Format checks: ISSN/ISBN token validity (normalized tokens of length 8 with digits/X)\n",
        "- Distribution snapshots: value counts or quantiles for selected variables\n",
        "- Risk indicators: normalized duplicate DOI (if a DOI column exists upstream), rows missing both `issn` and `isbn`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load final and run checks\n",
        "final_df = pd.read_parquet(OUTPUT_PARQUET, engine='pyarrow')\n",
        "print('Final shape (reloaded):', final_df.shape)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Expanded missingness for variable-related columns ONLY (treat empty string '' as NA)\n",
        "var_cols = [\n",
        "    # Invisibility block\n",
        "    'invisibility','times_cited','date',\n",
        "    # Geographic/Institutional (include names/types as requested)\n",
        "    'research_org_country_names','research_org_names','research_org_types',\n",
        "    # Topical\n",
        "    'concepts','concepts_scores',\n",
        "    # Disciplinary\n",
        "    'issn','isbn','disciplinary',\n",
        "    # OA\n",
        "    'open_access',\n",
        "    # Controls\n",
        "    'document_type','type','authors_count','reference_ids','referenced_pubs'\n",
        "]\n",
        "var_cols = [c for c in var_cols if c in final_df.columns]\n",
        "\n",
        "def is_blank(s: pd.Series) -> pd.Series:\n",
        "    return s.isna() | s.astype(str).str.strip().eq('')\n",
        "\n",
        "var_na_counts = {c: int(is_blank(final_df[c]).sum()) for c in var_cols}\n",
        "var_na_ratio = {c: round(var_na_counts[c] / len(final_df), 4) for c in var_cols}\n",
        "var_missing = (\n",
        "    pd.DataFrame({'na_count': pd.Series(var_na_counts), 'na_ratio': pd.Series(var_na_ratio)})\n",
        "      .sort_values('na_ratio', ascending=False)\n",
        ")\n",
        "print('\\nVariable columns missingness (NA + empty strings), sorted by na_ratio (final output columns only):')\n",
        "print(var_missing)\n",
        "\n",
        "# Key constraints\n",
        "print('\\nID uniqueness check:')\n",
        "print('Total ids:', final_df['id'].size, 'Distinct ids:', final_df['id'].nunique())\n",
        "\n",
        "# Domain checks\n",
        "if 'invisibility' in final_df.columns:\n",
        "    print('\\nInvisibility value counts:')\n",
        "    print(final_df['invisibility'].value_counts(dropna=False))\n",
        "\n",
        "if 'times_cited' in final_df.columns:\n",
        "    # Coerce to numeric for robust comparison and quantiles\n",
        "    tc = pd.to_numeric(final_df['times_cited'], errors='coerce')\n",
        "    print('\\nTimes cited quantiles:')\n",
        "    print(tc.describe(percentiles=[0.5,0.9,0.99]))\n",
        "    neg_tc = (tc.dropna() < 0).sum()\n",
        "    print('Negative times_cited count:', int(neg_tc))\n",
        "\n",
        "if 'authors_count' in final_df.columns:\n",
        "    ac = pd.to_numeric(final_df['authors_count'], errors='coerce')\n",
        "    print('\\nAuthors count quantiles:')\n",
        "    print(ac.describe(percentiles=[0.5,0.9,0.99]))\n",
        "    neg_ac = (ac.dropna() < 0).sum()\n",
        "    print('Negative authors_count count:', int(neg_ac))\n",
        "\n",
        "# Format checks for identifiers\n",
        "import re\n",
        "TOK = re.compile(r'[0-9A-Za-z]+')\n",
        "\n",
        "def norm_tokens(val: object) -> list[str]:\n",
        "    if pd.isna(val):\n",
        "        return []\n",
        "    tokens = TOK.findall(str(val))\n",
        "    tokens = [re.sub(r'[^0-9X]', '', t.upper()) for t in tokens]\n",
        "    return [t for t in tokens if t]\n",
        "\n",
        "if ('issn' in final_df.columns) or ('isbn' in final_df.columns):\n",
        "    def valid_issn_like(x: object) -> bool:\n",
        "        toks = norm_tokens(x)\n",
        "        # Accept any token with length 8\n",
        "        return any(len(t) == 8 for t in toks)\n",
        "    both_missing = 0\n",
        "    if ('issn' in final_df.columns) and ('isbn' in final_df.columns):\n",
        "        both_missing = (is_blank(final_df['issn']) & is_blank(final_df['isbn'])).sum()\n",
        "    print('\\nRows missing both issn and isbn (NA + empty strings):', int(both_missing))\n",
        "    if 'issn' in final_df.columns:\n",
        "        print('Share of rows with a token that looks like ISSN:', round(final_df['issn'].map(valid_issn_like).mean(), 4))\n",
        "\n",
        "# Risk indicators (base-level DOI duplicates if DOI exists upstream)\n",
        "if 'doi' in pf.schema.names:\n",
        "    doi_base = pd.read_parquet(PARQUET_INPUT, engine='pyarrow', columns=['id', 'doi'])\n",
        "    doi_base['doi_norm'] = (\n",
        "        doi_base['doi'].astype(str)\n",
        "        .str.lower()\n",
        "        .str.replace(r'^https?://(dx\\.)?doi\\.org/', '', regex=True)\n",
        "        .str.replace(r'\\s+', '', regex=True)\n",
        "    )\n",
        "    dup = doi_base[\n",
        "        doi_base['doi_norm'].ne('') & doi_base['doi_norm'].notna()\n",
        "    ].duplicated('doi_norm', keep=False).sum()\n",
        "    print('\\nDuplicate DOI (normalized) at base level:', int(dup))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
