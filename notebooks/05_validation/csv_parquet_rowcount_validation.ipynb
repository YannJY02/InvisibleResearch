{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CSV vs Parquet Row Count Validation\n",
        "\n",
        "This notebook validates that the merged Parquet contains all rows from the raw CSV files by comparing:\n",
        "- Total rows across all CSVs in `data/raw/openalex_data/`\n",
        "- Total rows in `data/processed/openalex_merged.csv`\n",
        "- Total rows in `data/processed/openalex_merged.parquet`\n",
        "\n",
        "It uses robust, streaming-friendly methods suitable for large files.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import os\n",
        "import csv\n",
        "from pathlib import Path\n",
        "from typing import Iterable, List, Tuple\n",
        "\n",
        "RAW_DIR = Path('/Users/yann.jy/InvisibleResearch/data/raw/openalex_data')\n",
        "MERGED_CSV = Path('/Users/yann.jy/InvisibleResearch/data/processed/openalex_merged.csv')\n",
        "MERGED_PARQUET = Path('/Users/yann.jy/InvisibleResearch/data/processed/openalex_merged.parquet')\n",
        "\n",
        "\n",
        "def list_csv_files(root: Path) -> List[Path]:\n",
        "    files: List[Path] = []\n",
        "    for dirpath, _dirnames, filenames in os.walk(root):\n",
        "        for fn in filenames:\n",
        "            if fn.lower().endswith('.csv'):\n",
        "                files.append(Path(dirpath) / fn)\n",
        "    files.sort()\n",
        "    return files\n",
        "\n",
        "\n",
        "def count_csv_rows(file_path: Path) -> int:\n",
        "    \"\"\"Count data rows in a CSV (excluding header), robust to quoted newlines.\"\"\"\n",
        "    total = 0\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='replace', newline='') as f:\n",
        "        reader = csv.reader(f, delimiter=',', quotechar='\"', doublequote=True, escapechar='\\\\')\n",
        "        _ = next(reader, None)  # skip header\n",
        "        for _row in reader:\n",
        "            total += 1\n",
        "    return total\n",
        "\n",
        "\n",
        "def count_parquet_rows(parquet_path: Path) -> int:\n",
        "    \"\"\"Count rows in Parquet using metadata; falls back to DuckDB if needed.\"\"\"\n",
        "    try:\n",
        "        import pyarrow.parquet as pq  # type: ignore\n",
        "        pf = pq.ParquetFile(str(parquet_path))\n",
        "        return pf.metadata.num_rows\n",
        "    except Exception:\n",
        "        try:\n",
        "            import duckdb  # type: ignore\n",
        "            con = duckdb.connect()\n",
        "            return con.execute(f\"SELECT COUNT(*) FROM read_parquet('{parquet_path.as_posix()}')\").fetchone()[0]\n",
        "        except Exception as e2:\n",
        "            raise RuntimeError(f\"Failed to count Parquet rows: {e2}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute counts + per-file breakdown and export\n",
        "raw_files = list_csv_files(RAW_DIR)\n",
        "\n",
        "# Per-file counts\n",
        "file_counts = []\n",
        "raw_total = 0\n",
        "for fp in raw_files:\n",
        "    rows = count_csv_rows(fp)\n",
        "    file_counts.append((fp.relative_to(RAW_DIR).as_posix(), rows))\n",
        "    raw_total += rows\n",
        "\n",
        "# Print aligned table\n",
        "name_width = max((len(name) for name, _ in file_counts), default=10)\n",
        "print(f\"{'CSV File'.ljust(name_width)} | Rows\")\n",
        "print('-' * (name_width + 7))\n",
        "for name, cnt in file_counts:\n",
        "    print(f\"{name.ljust(name_width)} | {cnt}\")\n",
        "\n",
        "# Export to CSV\n",
        "from pathlib import Path\n",
        "import csv as _csv\n",
        "OUTPUT_CSV = Path('/Users/yann.jy/InvisibleResearch/outputs/reports/openalex_csv_row_counts_by_file.csv')\n",
        "OUTPUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
        "with open(OUTPUT_CSV, 'w', encoding='utf-8', newline='') as f:\n",
        "    w = _csv.writer(f)\n",
        "    w.writerow(['file', 'rows'])\n",
        "    for name, cnt in file_counts:\n",
        "        w.writerow([name, cnt])\n",
        "print(f\"\\nExported per-file counts to: {OUTPUT_CSV}\")\n",
        "\n",
        "# Previous totals and parity checks\n",
        "merged_csv_rows = count_csv_rows(MERGED_CSV)\n",
        "parquet_rows = count_parquet_rows(MERGED_PARQUET)\n",
        "\n",
        "print('raw_csv_total_rows:', raw_total)\n",
        "print('merged_csv_rows:', merged_csv_rows)\n",
        "print('parquet_rows:', parquet_rows)\n",
        "print('raw_vs_merged_equal:', raw_total == merged_csv_rows)\n",
        "print('merged_vs_parquet_equal:', merged_csv_rows == parquet_rows)\n",
        "print('raw_vs_parquet_equal:', raw_total == parquet_rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Per-file CSV row counts breakdown and export\n",
        "from __future__ import annotations\n",
        "import csv\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "\n",
        "RAW_DIR = Path('/Users/yann.jy/InvisibleResearch/data/raw/openalex_data')\n",
        "OUTPUT_CSV = Path('/Users/yann.jy/InvisibleResearch/outputs/reports/openalex_csv_row_counts_by_file.csv')\n",
        "\n",
        "# Reuse count_csv_rows if already defined in previous cells; otherwise define here\n",
        "try:\n",
        "    count_csv_rows  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    def count_csv_rows(file_path: Path) -> int:\n",
        "        total = 0\n",
        "        with open(file_path, 'r', encoding='utf-8', errors='replace', newline='') as f:\n",
        "            reader = csv.reader(f, delimiter=',', quotechar='\"', doublequote=True, escapechar='\\\\')\n",
        "            _ = next(reader, None)\n",
        "            for _row in reader:\n",
        "                total += 1\n",
        "        return total\n",
        "\n",
        "# Collect per-file counts\n",
        "file_counts: List[Tuple[str, int]] = []\n",
        "for fp in sorted(RAW_DIR.rglob('*.csv')):\n",
        "    relative = fp.relative_to(RAW_DIR).as_posix()\n",
        "    file_counts.append((relative, count_csv_rows(fp)))\n",
        "\n",
        "# Print as a simple aligned table\n",
        "name_width = max((len(name) for name, _ in file_counts), default=10)\n",
        "print(f\"{'CSV File'.ljust(name_width)} | Rows\")\n",
        "print('-' * (name_width + 7))\n",
        "for name, cnt in file_counts:\n",
        "    print(f\"{name.ljust(name_width)} | {cnt}\")\n",
        "\n",
        "# Export to CSV\n",
        "OUTPUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
        "with open(OUTPUT_CSV, 'w', encoding='utf-8', newline='') as f:\n",
        "    w = csv.writer(f)\n",
        "    w.writerow(['file', 'rows'])\n",
        "    for name, cnt in file_counts:\n",
        "        w.writerow([name, cnt])\n",
        "\n",
        "print(f\"\\nExported per-file counts to: {OUTPUT_CSV}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Per-file CSV row counts breakdown and export (appended)\n",
        "from pathlib import Path\n",
        "import csv as _csv\n",
        "\n",
        "RAW_DIR = Path('/Users/yann.jy/InvisibleResearch/data/raw/openalex_data')\n",
        "OUTPUT_CSV = Path('/Users/yann.jy/InvisibleResearch/outputs/reports/openalex_csv_row_counts_by_file.csv')\n",
        "\n",
        "# Build per-file counts using existing helpers\n",
        "file_counts = []\n",
        "for fp in list_csv_files(RAW_DIR):\n",
        "    rel = fp.relative_to(RAW_DIR).as_posix()\n",
        "    cnt = count_csv_rows(fp)\n",
        "    file_counts.append((rel, cnt))\n",
        "\n",
        "# Print table\n",
        "name_width = max((len(n) for n, _ in file_counts), default=10)\n",
        "print(f\"{'CSV File'.ljust(name_width)} | Rows\")\n",
        "print('-' * (name_width + 7))\n",
        "for name, cnt in file_counts:\n",
        "    print(f\"{name.ljust(name_width)} | {cnt}\")\n",
        "\n",
        "# Export\n",
        "OUTPUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
        "with open(OUTPUT_CSV, 'w', encoding='utf-8', newline='') as f:\n",
        "    w = _csv.writer(f)\n",
        "    w.writerow(['file', 'rows'])\n",
        "    w.writerows(file_counts)\n",
        "\n",
        "print(f\"\\nExported per-file counts to: {OUTPUT_CSV}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Per-file CSV row counts breakdown and export\n",
        "from __future__ import annotations\n",
        "import csv\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "\n",
        "RAW_DIR = Path('/Users/yann.jy/InvisibleResearch/data/raw/openalex_data')\n",
        "OUTPUT_CSV = Path('/Users/yann.jy/InvisibleResearch/outputs/reports/openalex_csv_row_counts_by_file.csv')\n",
        "\n",
        "# Reuse count_csv_rows if already defined in previous cells; otherwise define here\n",
        "try:\n",
        "    count_csv_rows  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    def count_csv_rows(file_path: Path) -> int:\n",
        "        total = 0\n",
        "        with open(file_path, 'r', encoding='utf-8', errors='replace', newline='') as f:\n",
        "            reader = csv.reader(f, delimiter=',', quotechar='\"', doublequote=True, escapechar='\\\\')\n",
        "            _ = next(reader, None)\n",
        "            for _row in reader:\n",
        "                total += 1\n",
        "        return total\n",
        "\n",
        "# Collect per-file counts\n",
        "file_counts: List[Tuple[str, int]] = []\n",
        "for fp in sorted(RAW_DIR.rglob('*.csv')):\n",
        "    relative = fp.relative_to(RAW_DIR).as_posix()\n",
        "    file_counts.append((relative, count_csv_rows(fp)))\n",
        "\n",
        "# Print as a simple aligned table\n",
        "name_width = max((len(name) for name, _ in file_counts), default=10)\n",
        "print(f\"{'CSV File'.ljust(name_width)} | Rows\")\n",
        "print('-' * (name_width + 7))\n",
        "for name, cnt in file_counts:\n",
        "    print(f\"{name.ljust(name_width)} | {cnt}\")\n",
        "\n",
        "# Export to CSV\n",
        "OUTPUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
        "with open(OUTPUT_CSV, 'w', encoding='utf-8', newline='') as f:\n",
        "    w = csv.writer(f)\n",
        "    w.writerow(['file', 'rows'])\n",
        "    for name, cnt in file_counts:\n",
        "        w.writerow([name, cnt])\n",
        "\n",
        "print(f\"\\nExported per-file counts to: {OUTPUT_CSV}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: year-range check for 1925-1999 on both CSV and Parquet\n",
        "from collections import Counter\n",
        "\n",
        "def csv_year_distribution(file_path: Path, year_col: str = 'publication_year'):\n",
        "    total = 0\n",
        "    counter = Counter()\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='replace', newline='') as f:\n",
        "        reader = csv.reader(f, delimiter=',', quotechar='\"', doublequote=True, escapechar='\\\\')\n",
        "        header = next(reader)\n",
        "        name_to_idx = {name.strip(): i for i, name in enumerate(header)}\n",
        "        yi = name_to_idx.get(year_col)\n",
        "        for row in reader:\n",
        "            total += 1\n",
        "            if yi is not None and yi < len(row):\n",
        "                v = row[yi].strip()\n",
        "                if v.isdigit():\n",
        "                    y = int(v)\n",
        "                    if 1900 <= y <= 2100:\n",
        "                        counter[y] += 1\n",
        "    return total, counter\n",
        "\n",
        "raw1925_total, raw1925_counter = csv_year_distribution(MERGED_CSV)\n",
        "print('Merged CSV 1925-1999 sum:', sum(c for y, c in raw1925_counter.items() if 1925 <= y <= 1999))\n",
        "\n",
        "try:\n",
        "    import duckdb\n",
        "    con = duckdb.connect()\n",
        "    pq_yr = con.execute(\n",
        "        f\"\"\"\n",
        "        WITH t AS (\n",
        "          SELECT try_cast(publication_year AS INTEGER) AS py\n",
        "          FROM read_parquet('{MERGED_PARQUET.as_posix()}')\n",
        "        )\n",
        "        SELECT py, COUNT(*) AS c FROM t WHERE py BETWEEN 1925 AND 1999 GROUP BY py ORDER BY py\n",
        "        \"\"\"\n",
        "    ).fetchall()\n",
        "    print('Parquet 1925-1999 sum:', sum(c for _, c in pq_yr))\n",
        "except Exception as e:\n",
        "    print('DuckDB not available for year-range check:', e)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
