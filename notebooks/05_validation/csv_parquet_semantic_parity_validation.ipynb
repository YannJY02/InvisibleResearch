{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CSV â†” Parquet Semantic Parity Validation\n",
        "\n",
        "This notebook checks for semantic integrity across the pipeline:\n",
        "- Raw CSVs in `/Users/yann.jy/InvisibleResearch/data/raw/openalex_data/`\n",
        "- Merged CSV at `/Users/yann.jy/InvisibleResearch/data/processed/openalex_merged.csv`\n",
        "- Parquet at `/Users/yann.jy/InvisibleResearch/data/processed/openalex_merged.parquet`\n",
        "\n",
        "It detects potential data loss, misalignment, or unintended conversion of meaningful tokens into NULL/NA during merge/convert steps. It does not modify the data, only reports findings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "from collections import defaultdict, Counter\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Iterable, Set\n",
        "\n",
        "RAW_DIR = Path('/Users/yann.jy/InvisibleResearch/data/raw/openalex_data')\n",
        "MERGED_CSV = Path('/Users/yann.jy/InvisibleResearch/data/processed/openalex_merged.csv')\n",
        "MERGED_PARQUET = Path('/Users/yann.jy/InvisibleResearch/data/processed/openalex_merged.parquet')\n",
        "REPORTS_DIR = Path('/Users/yann.jy/InvisibleResearch/outputs/reports')\n",
        "\n",
        "SUSPICIOUS_TOKENS: List[str] = [\n",
        "    \"\", \"NA\", \"N.A.\", \"N/A\", \"NULL\", \"NaN\", \"\\\\N\", \"None\", \"null\", \"nan\", \"Na\"\n",
        "]\n",
        "\n",
        "# Utility to list raw CSV files\n",
        "\n",
        "def list_csv_files(root: Path) -> List[Path]:\n",
        "    files: List[Path] = []\n",
        "    for dirpath, _dirnames, filenames in os.walk(root):\n",
        "        for fn in filenames:\n",
        "            if fn.lower().endswith('.csv'):\n",
        "                files.append(Path(dirpath) / fn)\n",
        "    files.sort()\n",
        "    return files\n",
        "\n",
        "# Robust CSV reader factory\n",
        "\n",
        "def open_csv_reader(file_path: Path) -> Iterable[List[str]]:\n",
        "    f = open(file_path, 'r', encoding='utf-8', errors='replace', newline='')\n",
        "    reader = csv.reader(f, delimiter=',', quotechar='\"', doublequote=True, escapechar='\\\\')\n",
        "    return f, reader\n",
        "\n",
        "# Read header and data row count only\n",
        "\n",
        "def read_header_and_count_rows(file_path: Path) -> Tuple[List[str], int]:\n",
        "    total = 0\n",
        "    f, reader = open_csv_reader(file_path)\n",
        "    try:\n",
        "        header = next(reader, None)\n",
        "        if header is None:\n",
        "            return [], 0\n",
        "        for _ in reader:\n",
        "            total += 1\n",
        "        return [c.strip() for c in header], total\n",
        "    finally:\n",
        "        f.close()\n",
        "\n",
        "# Count suspicious tokens per-column for a single CSV file (only for columns present)\n",
        "\n",
        "def count_tokens_in_csv(file_path: Path, columns: List[str], tokens: Set[str]) -> Tuple[Dict[str, Counter], int]:\n",
        "    per_col: Dict[str, Counter] = {c: Counter() for c in columns}\n",
        "    row_count = 0\n",
        "    f, reader = open_csv_reader(file_path)\n",
        "    try:\n",
        "        header = next(reader, None)\n",
        "        if header is None:\n",
        "            return per_col, 0\n",
        "        header = [c.strip() for c in header]\n",
        "        name_to_idx = {name: i for i, name in enumerate(header)}\n",
        "        present_columns = [c for c in columns if c in name_to_idx]\n",
        "\n",
        "        for row in reader:\n",
        "            row_count += 1\n",
        "            row_len = len(row)\n",
        "            for col in present_columns:\n",
        "                idx = name_to_idx[col]\n",
        "                if idx < row_len:\n",
        "                    val = row[idx]\n",
        "                else:\n",
        "                    # Truncated row; treat as empty string to avoid IndexError\n",
        "                    val = \"\"\n",
        "                if val in tokens:\n",
        "                    per_col[col][val] += 1\n",
        "        return per_col, row_count\n",
        "    finally:\n",
        "        f.close()\n",
        "\n",
        "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print('Prepared environment. Raw dir exists:', RAW_DIR.exists())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Discover raw CSV files and build union of columns\n",
        "raw_files = list_csv_files(RAW_DIR)\n",
        "print(f\"Discovered {len(raw_files)} raw CSV files\")\n",
        "\n",
        "union_columns: Set[str] = set()\n",
        "per_file_headers: Dict[str, List[str]] = {}\n",
        "per_file_rowcounts: Dict[str, int] = {}\n",
        "\n",
        "for fp in raw_files:\n",
        "    header, nrows = read_header_and_count_rows(fp)\n",
        "    if header:\n",
        "        union_columns.update(header)\n",
        "    per_file_headers[fp.as_posix()] = header\n",
        "    per_file_rowcounts[fp.as_posix()] = nrows\n",
        "\n",
        "union_columns_sorted: List[str] = sorted(union_columns)\n",
        "print(f\"Union columns: {len(union_columns_sorted)}\")\n",
        "\n",
        "# 2) Scan raw CSVs for suspicious token counts per column\n",
        "from copy import deepcopy\n",
        "\n",
        "raw_token_counts: Dict[str, Counter] = {c: Counter() for c in union_columns_sorted}\n",
        "raw_total_rows = 0\n",
        "\n",
        "for fp in raw_files:\n",
        "    per_col_counts, nrows = count_tokens_in_csv(fp, union_columns_sorted, set(SUSPICIOUS_TOKENS))\n",
        "    raw_total_rows += nrows\n",
        "    for col in union_columns_sorted:\n",
        "        raw_token_counts[col].update(per_col_counts[col])\n",
        "\n",
        "print('Raw total rows (data rows across files):', raw_total_rows)\n",
        "\n",
        "# 3) Read merged CSV and compute suspicious token counts per column\n",
        "merged_token_counts: Dict[str, Counter] = {c: Counter() for c in union_columns_sorted}\n",
        "merged_total_rows = 0\n",
        "f_m, rdr_m = open_csv_reader(MERGED_CSV)\n",
        "try:\n",
        "    header_m = next(rdr_m, None)\n",
        "    if not header_m:\n",
        "        raise RuntimeError('Merged CSV has no header')\n",
        "    header_m = [c.strip() for c in header_m]\n",
        "    if set(header_m) != set(union_columns_sorted):\n",
        "        print('WARNING: merged CSV columns differ from union columns sizes:', len(header_m), 'vs', len(union_columns_sorted))\n",
        "    name_to_idx_m = {name: i for i, name in enumerate(header_m)}\n",
        "    for row in rdr_m:\n",
        "        merged_total_rows += 1\n",
        "        row_len = len(row)\n",
        "        for col in union_columns_sorted:\n",
        "            idx = name_to_idx_m.get(col)\n",
        "            val = row[idx] if idx is not None and idx < row_len else \"\"\n",
        "            if val in SUSPICIOUS_TOKENS:\n",
        "                merged_token_counts[col][val] += 1\n",
        "finally:\n",
        "    f_m.close()\n",
        "\n",
        "print('Merged CSV rows:', merged_total_rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure duckdb is available in this kernel and provide identifier escaping\n",
        "try:\n",
        "    import duckdb as _duckdb\n",
        "except ModuleNotFoundError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', 'duckdb>=0.10.0'])\n",
        "    import duckdb as _duckdb\n",
        "\n",
        "# Provide escape_identifier if missing\n",
        "def _escape_identifier_py(name: str) -> str:\n",
        "    s = '\"' + str(name).replace('\"', '\"\"') + '\"'\n",
        "    return s\n",
        "\n",
        "if not hasattr(_duckdb, 'escape_identifier'):\n",
        "    _duckdb.escape_identifier = _escape_identifier_py  # type: ignore\n",
        "\n",
        "print('duckdb version:', getattr(_duckdb, '__version__', 'unknown'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4) Parquet: NULL counts and suspicious string counts via DuckDB\n",
        "import duckdb\n",
        "con = duckdb.connect()\n",
        "\n",
        "# Load schema (column names) from merged CSV to enforce order\n",
        "csv_cols = union_columns_sorted\n",
        "\n",
        "# Construct per-column NULL count queries and token counts\n",
        "susp_list = SUSPICIOUS_TOKENS\n",
        "\n",
        "# Build a temporary view for Parquet\n",
        "parquet_path = MERGED_PARQUET.as_posix().replace(\"'\", \"''\")\n",
        "con.execute(f\"CREATE OR REPLACE VIEW v_parquet AS SELECT * FROM read_parquet('{parquet_path}')\")\n",
        "\n",
        "# Get NULL counts\n",
        "null_counts = {}\n",
        "for col in csv_cols:\n",
        "    q = f\"SELECT COUNT(*) FROM v_parquet WHERE {duckdb.escape_identifier(col)} IS NULL\"\n",
        "    null_counts[col] = con.execute(q).fetchone()[0]\n",
        "\n",
        "# Count suspicious tokens that survived as strings in Parquet\n",
        "parquet_token_counts = {c: Counter() for c in csv_cols}\n",
        "for col in csv_cols:\n",
        "    for tok in susp_list:\n",
        "        # We count literal matches; for empty string we use col = ''\n",
        "        if tok == \"\":\n",
        "            q = f\"SELECT COUNT(*) FROM v_parquet WHERE {duckdb.escape_identifier(col)} = ''\"\n",
        "        else:\n",
        "            # Compare as string; DuckDB will cast to VARCHAR\n",
        "            q = f\"SELECT COUNT(*) FROM v_parquet WHERE {duckdb.escape_identifier(col)} = ?\"\n",
        "        cnt = con.execute(q, [tok] if tok != \"\" else []).fetchone()[0]\n",
        "        if cnt:\n",
        "            parquet_token_counts[col][tok] = cnt\n",
        "\n",
        "print('Parquet counts computed.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5) Build discrepancy reports\n",
        "from math import isfinite\n",
        "import pandas as pd\n",
        "\n",
        "# Compute missing-column-induced empty counts expected in merged CSV\n",
        "missing_column_rows: Dict[str, int] = {c: 0 for c in union_columns_sorted}\n",
        "for col in union_columns_sorted:\n",
        "    miss_total = 0\n",
        "    for fpath, hdr in per_file_headers.items():\n",
        "        if col not in hdr:\n",
        "            miss_total += per_file_rowcounts[fpath]\n",
        "    missing_column_rows[col] = miss_total\n",
        "\n",
        "# Helper to get count safely\n",
        "get_raw = lambda c, t: int(raw_token_counts.get(c, Counter()).get(t, 0))\n",
        "get_mrg = lambda c, t: int(merged_token_counts.get(c, Counter()).get(t, 0))\n",
        "get_par = lambda c, t: int(parquet_token_counts.get(c, Counter()).get(t, 0))\n",
        "get_null = lambda c: int(null_counts.get(c, 0))\n",
        "\n",
        "# Long-form per-token report\n",
        "rows_token: List[Dict[str, object]] = []\n",
        "for col in union_columns_sorted:\n",
        "    for tok in SUSPICIOUS_TOKENS:\n",
        "        raw_cnt = get_raw(col, tok)\n",
        "        mrg_cnt = get_mrg(col, tok)\n",
        "        par_cnt = get_par(col, tok)\n",
        "        null_cnt = get_null(col)\n",
        "        miss_rows = missing_column_rows[col]\n",
        "        expected_merged_empty = (get_raw(col, \"\") + miss_rows) if tok == \"\" else None\n",
        "        lost_to_parquet = max(0, mrg_cnt - par_cnt)\n",
        "        rows_token.append({\n",
        "            'column': col,\n",
        "            'token': tok,\n",
        "            'raw_count': raw_cnt,\n",
        "            'merged_count': mrg_cnt,\n",
        "            'parquet_string_count': par_cnt,\n",
        "            'parquet_null_count': null_cnt,\n",
        "            'missing_column_rows': miss_rows,\n",
        "            'expected_merged_empty': expected_merged_empty,\n",
        "            'lost_to_parquet': lost_to_parquet,\n",
        "            'flag_token_loss': lost_to_parquet > 0,\n",
        "        })\n",
        "\n",
        "df_token = pd.DataFrame(rows_token)\n",
        "\n",
        "# Per-column summary\n",
        "summary_rows: List[Dict[str, object]] = []\n",
        "for col in union_columns_sorted:\n",
        "    merged_empty = get_mrg(col, \"\")\n",
        "    parquet_empty_str = get_par(col, \"\")\n",
        "    empty_lost_to_null = max(0, merged_empty - parquet_empty_str)\n",
        "    null_cnt = get_null(col)\n",
        "    miss_rows = missing_column_rows[col]\n",
        "    expected_merged_empty = get_raw(col, \"\") + miss_rows\n",
        "    merged_empty_diff_vs_expected = merged_empty - expected_merged_empty\n",
        "    # Sum of losses across all suspicious tokens\n",
        "    sum_lost_tokens = int(sum(max(0, get_mrg(col, tok) - get_par(col, tok)) for tok in SUSPICIOUS_TOKENS))\n",
        "    # Decide flags\n",
        "    flag_null_inflation = null_cnt > empty_lost_to_null\n",
        "    flag_merged_empty_mismatch = merged_empty_diff_vs_expected != 0\n",
        "    flagged = (sum_lost_tokens > 0) or flag_null_inflation or flag_merged_empty_mismatch\n",
        "\n",
        "    # Top lost token\n",
        "    top_tok = None\n",
        "    top_loss = 0\n",
        "    for tok in SUSPICIOUS_TOKENS:\n",
        "        loss = max(0, get_mrg(col, tok) - get_par(col, tok))\n",
        "        if loss > top_loss:\n",
        "            top_loss = loss\n",
        "            top_tok = tok\n",
        "\n",
        "    summary_rows.append({\n",
        "        'column': col,\n",
        "        'parquet_null_count': null_cnt,\n",
        "        'merged_empty_count': merged_empty,\n",
        "        'parquet_empty_as_string_count': parquet_empty_str,\n",
        "        'empty_lost_to_null_min': empty_lost_to_null,\n",
        "        'missing_column_rows': miss_rows,\n",
        "        'expected_merged_empty': expected_merged_empty,\n",
        "        'merged_empty_diff_vs_expected': merged_empty_diff_vs_expected,\n",
        "        'sum_lost_suspicious_tokens': sum_lost_tokens,\n",
        "        'flag_null_inflation': flag_null_inflation,\n",
        "        'flag_merged_empty_mismatch': flag_merged_empty_mismatch,\n",
        "        'flagged': flagged,\n",
        "        'top_lost_token': top_tok,\n",
        "        'top_lost_token_count': top_loss,\n",
        "    })\n",
        "\n",
        "df_summary = pd.DataFrame(summary_rows).sort_values(['flagged','parquet_null_count','sum_lost_suspicious_tokens'], ascending=[False, False, False])\n",
        "\n",
        "# Export\n",
        "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "per_token_csv = REPORTS_DIR / 'semantic_parity_per_token.csv'\n",
        "per_col_csv = REPORTS_DIR / 'semantic_parity_summary_by_column.csv'\n",
        "df_token.to_csv(per_token_csv, index=False)\n",
        "df_summary.to_csv(per_col_csv, index=False)\n",
        "\n",
        "print('Wrote reports:')\n",
        "print(' -', per_token_csv)\n",
        "print(' -', per_col_csv)\n",
        "\n",
        "# Display top suspicious columns\n",
        "df_summary.head(20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6) Row-level spot checks for misalignment or token-to-NULL coercion\n",
        "\n",
        "import re\n",
        "\n",
        "SAMPLE_SIZE = 2000\n",
        "SAMPLE_COLUMNS: List[str] = []  # leave empty to auto-select a few columns\n",
        "\n",
        "# Decide join key\n",
        "candidate_key = 'id'\n",
        "\n",
        "# Inspect merged CSV header for key presence\n",
        "f_tmp, rdr_tmp = open_csv_reader(MERGED_CSV)\n",
        "try:\n",
        "    header_tmp = next(rdr_tmp, None)\n",
        "    if not header_tmp:\n",
        "        raise RuntimeError('Merged CSV has no header')\n",
        "    header_tmp = [c.strip() for c in header_tmp]\n",
        "finally:\n",
        "    f_tmp.close()\n",
        "\n",
        "join_by_id = candidate_key in header_tmp\n",
        "\n",
        "# Pick sample columns\n",
        "if SAMPLE_COLUMNS:\n",
        "    cols_for_sample = [c for c in SAMPLE_COLUMNS if c in header_tmp][:10]\n",
        "else:\n",
        "    # Auto-pick up to 10: include id if present, plus top flagged columns\n",
        "    cols_for_sample = ([] if not join_by_id else [candidate_key])\n",
        "    top_cols = df_summary.sort_values(['flagged','sum_lost_suspicious_tokens','parquet_null_count'], ascending=[False, False, False])['column'].tolist()\n",
        "    for c in top_cols:\n",
        "        if c not in cols_for_sample:\n",
        "            cols_for_sample.append(c)\n",
        "        if len(cols_for_sample) >= 10:\n",
        "            break\n",
        "\n",
        "print('Join by id:', join_by_id)\n",
        "print('Sample columns:', cols_for_sample)\n",
        "\n",
        "# Alias sanitizer to avoid dots and special chars in SQL aliases\n",
        "sanitize = lambda s: re.sub(r'[^A-Za-z0-9_]', '_', s)\n",
        "\n",
        "# Build sampling queries using DuckDB to compare merged CSV vs Parquet\n",
        "csv_path = MERGED_CSV.as_posix().replace(\"'\", \"''\")\n",
        "con.execute(\"DROP VIEW IF EXISTS v_csv\")\n",
        "con.execute(f\"CREATE VIEW v_csv AS SELECT * FROM read_csv('{csv_path}', AUTO_DETECT=TRUE, HEADER=TRUE)\")\n",
        "\n",
        "if join_by_id:\n",
        "    # Ensure uniqueness of id in both sides (best-effort check)\n",
        "    id_dups_csv = con.execute(\"SELECT COUNT(*) FROM (SELECT id, COUNT(*) c FROM v_csv GROUP BY 1 HAVING c>1)\").fetchone()[0]\n",
        "    id_dups_par = con.execute(\"SELECT COUNT(*) FROM (SELECT id, COUNT(*) c FROM v_parquet GROUP BY 1 HAVING c>1)\").fetchone()[0]\n",
        "    print('CSV duplicate ids:', id_dups_csv, 'Parquet duplicate ids:', id_dups_par)\n",
        "\n",
        "    select_cols = \", \".join([duckdb.escape_identifier(c) for c in cols_for_sample])\n",
        "    csv_aliases = {c: f\"csv__{sanitize(c)}\" for c in cols_for_sample if c != 'id'}\n",
        "    par_aliases = {c: f\"parquet__{sanitize(c)}\" for c in cols_for_sample if c != 'id'}\n",
        "\n",
        "    q = f\"\"\"\n",
        "    WITH s AS (\n",
        "      SELECT {select_cols}\n",
        "      FROM v_csv\n",
        "      USING SAMPLE {SAMPLE_SIZE} ROWS\n",
        "    )\n",
        "    SELECT s.id as key_id,\n",
        "           {', '.join([f's.{duckdb.escape_identifier(c)} as ' + duckdb.escape_identifier(csv_aliases[c]) for c in cols_for_sample if c!='id'])},\n",
        "           {', '.join([f'p.{duckdb.escape_identifier(c)} as ' + duckdb.escape_identifier(par_aliases[c]) for c in cols_for_sample if c!='id'])}\n",
        "    FROM s\n",
        "    LEFT JOIN v_parquet p ON p.id = s.id\n",
        "    \"\"\"\n",
        "else:\n",
        "    # Row-order based comparison using row_number\n",
        "    select_cols = \", \".join([duckdb.escape_identifier(c) for c in cols_for_sample])\n",
        "    csv_aliases = {c: f\"csv__{sanitize(c)}\" for c in cols_for_sample}\n",
        "    par_aliases = {c: f\"parquet__{sanitize(c)}\" for c in cols_for_sample}\n",
        "\n",
        "    q = f\"\"\"\n",
        "    WITH c AS (\n",
        "      SELECT row_number() OVER () AS rn, {select_cols}\n",
        "      FROM v_csv\n",
        "    ), p AS (\n",
        "      SELECT row_number() OVER () AS rn, {select_cols}\n",
        "      FROM v_parquet\n",
        "    ), s AS (\n",
        "      SELECT * FROM c USING SAMPLE {SAMPLE_SIZE} ROWS\n",
        "    )\n",
        "    SELECT s.rn as key_rn,\n",
        "           {', '.join([f's.{duckdb.escape_identifier(c)} as ' + duckdb.escape_identifier(csv_aliases[c]) for c in cols_for_sample])},\n",
        "           {', '.join([f'p.{duckdb.escape_identifier(c)} as ' + duckdb.escape_identifier(par_aliases[c]) for c in cols_for_sample])}\n",
        "    FROM s LEFT JOIN p ON p.rn = s.rn\n",
        "    \"\"\"\n",
        "\n",
        "sample_df = con.execute(q).fetch_df()\n",
        "\n",
        "sample_path = REPORTS_DIR / 'semantic_parity_row_samples.csv'\n",
        "sample_df.to_csv(sample_path, index=False)\n",
        "print('Wrote row-level sample to:', sample_path)\n",
        "\n",
        "# Heuristic mismatch markers\n",
        "mismatch_cols = []\n",
        "for c in cols_for_sample:\n",
        "    if c == candidate_key and join_by_id:\n",
        "        continue\n",
        "    csv_col = csv_aliases.get(c)\n",
        "    par_col = par_aliases.get(c)\n",
        "    if csv_col in sample_df.columns and par_col in sample_df.columns:\n",
        "        # Count where csv value is suspicious token but parquet differs (NULL or other)\n",
        "        mask = sample_df[csv_col].isin(SUSPICIOUS_TOKENS) & (sample_df[csv_col].astype(str) != sample_df[par_col].astype(str))\n",
        "        rate = float(mask.mean()) if len(sample_df) else 0.0\n",
        "        mismatch_cols.append((c, rate))\n",
        "\n",
        "print('Top mismatch columns (sample-based):', sorted(mismatch_cols, key=lambda x: x[1], reverse=True)[:10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to interpret the reports\n",
        "\n",
        "- The notebook writes two main reports under `/Users/yann.jy/InvisibleResearch/outputs/reports/`:\n",
        "  - `semantic_parity_summary_by_column.csv`: high-level per-column flags and counts.\n",
        "  - `semantic_parity_per_token.csv`: detailed counts for each suspicious token per column.\n",
        "  - `semantic_parity_row_samples.csv`: small sample comparing merged CSV vs Parquet values.\n",
        "- Focus on columns with `flagged = True`, large `sum_lost_suspicious_tokens`, or high `parquet_null_count` unexplained by `empty_lost_to_null_min`.\n",
        "- This notebook only detects issues and does not modify any data.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
