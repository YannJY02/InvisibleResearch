#!/usr/bin/env python3
"""
æ•°æ®ç®¡ç†æ¨¡å— - LLMéªŒè¯å®¡æ ¸ç³»ç»Ÿ
Data Manager Module for LLM Validation Suite

è´Ÿè´£æ•°æ®åŠ è½½ã€åˆ†å±‚æŠ½æ ·ã€è¿›åº¦ç®¡ç†ç­‰åŠŸèƒ½
"""

import os
import json
import random
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, asdict
from datetime import datetime

import pandas as pd
import pyarrow.parquet as pq
import pyarrow as pa
import yaml
import numpy as np


@dataclass
class ValidationRecord:
    """éªŒè¯è®°å½•æ•°æ®ç»“æ„"""
    record_id: str
    title: str
    original_creator: str
    processed_authors: str
    processed_affiliations: List[str]
    complexity_level: str
    author_count: int
    creator_length: int
    
    # éªŒè¯ç»“æœ
    validator_name: Optional[str] = None
    validation_timestamp: Optional[str] = None
    author_identification_score: Optional[int] = None
    author_separation_score: Optional[int] = None
    name_affiliation_score: Optional[int] = None
    name_formatting_score: Optional[int] = None
    overall_status: Optional[str] = None  # "correct", "incorrect", "partial"
    notes: Optional[str] = None
    external_verification: Optional[Dict] = None


class DataManager:
    """æ•°æ®ç®¡ç†å™¨ç±»"""
    
    def __init__(self, config_path: str = "scripts/05_validation/validation_config.yaml"):
        # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œä»é¡¹ç›®æ ¹ç›®å½•å¼€å§‹æŸ¥æ‰¾
        if not os.path.isabs(config_path):
            # å°è¯•ä»å½“å‰ç›®å½•ã€çˆ¶ç›®å½•ã€é¡¹ç›®æ ¹ç›®å½•æŸ¥æ‰¾
            possible_paths = [
                Path(config_path),
                Path("../") / config_path,
                Path("../../") / config_path,
                Path("../../../") / config_path
            ]
            
            for path in possible_paths:
                if path.exists():
                    self.config_path = path
                    break
            else:
                self.config_path = Path(config_path)
        else:
            self.config_path = Path(config_path)
            
        self.config = self._load_config()
        self.data_paths = self.config['data_paths']
        
        # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
        Path(self.data_paths['validation_results']).parent.mkdir(parents=True, exist_ok=True)
        
    def _load_config(self) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        with open(self.config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def load_source_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        åŠ è½½æºæ•°æ®
        è¿”å›: (è¾“å…¥æ•°æ®DataFrame, è¾“å‡ºæ•°æ®DataFrame)
        """
        input_df = pq.read_table(self.data_paths['input_file']).to_pandas()
        output_df = pq.read_table(self.data_paths['output_file']).to_pandas()
        
        return input_df, output_df
    
    def _classify_complexity(self, record: Dict) -> str:
        """æ ¹æ®é…ç½®æ ‡å‡†åˆ†ç±»å¤æ‚åº¦"""
        criteria = self.config['complexity_criteria']
        
        # å¤„ç†å¯èƒ½çš„åˆ—ååç¼€
        creator_key = 'creator_input' if 'creator_input' in record else 'creator'
        count_key = 'creator_count_input' if 'creator_count_input' in record else 'creator_count'
        affil_key = 'affiliations' if 'affiliations' in record else []
        
        creator_length = len(str(record[creator_key]))
        author_count = record[count_key]
        has_affiliations = len(record.get(affil_key, [])) > 0
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºç®€å•æ¡ˆä¾‹
        simple = criteria['simple']
        if (creator_length <= simple['max_length'] and 
            author_count <= simple['max_authors'] and
            has_affiliations == simple['has_affiliations']):
            return 'simple'
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºä¸­ç­‰å¤æ‚æ¡ˆä¾‹
        medium = criteria['medium']
        if (creator_length <= medium['max_length'] and 
            author_count <= medium['max_authors']):
            return 'medium'
        
        return 'complex'
    
    def prepare_validation_records(self) -> List[ValidationRecord]:
        """å‡†å¤‡éªŒè¯è®°å½•åˆ—è¡¨"""
        input_df, output_df = self.load_source_data()
        
        # åˆå¹¶æ•°æ®
        merged_df = pd.merge(input_df, output_df, on='id', suffixes=('_input', '_output'))
        
        records = []
        for _, row in merged_df.iterrows():
            complexity = self._classify_complexity(row)
            
            # å¤„ç†å¯èƒ½çš„åˆ—åå˜åŒ–
            title_key = 'title_input' if 'title_input' in merged_df.columns else 'title'
            creator_input_key = 'creator_input' if 'creator_input' in merged_df.columns else 'creator'
            count_input_key = 'creator_count_input' if 'creator_count_input' in merged_df.columns else 'creator_count'
            
            record = ValidationRecord(
                record_id=str(row['id']),
                title=str(row[title_key]),
                original_creator=str(row[creator_input_key]),
                processed_authors=str(row['authors_clean']),
                processed_affiliations=row['affiliations'] if isinstance(row['affiliations'], list) else [],
                complexity_level=complexity,
                author_count=int(row[count_input_key]),
                creator_length=len(str(row[creator_input_key]))
            )
            records.append(record)
        
        return records
    
    def stratified_sampling(self, records: List[ValidationRecord]) -> List[ValidationRecord]:
        """åˆ†å±‚æŠ½æ ·"""
        sampling_config = self.config['validation_modes']['stratified_sampling']
        
        if not sampling_config['enabled']:
            return records
        
        # æŒ‰å¤æ‚åº¦åˆ†ç»„
        groups = {'simple': [], 'medium': [], 'complex': []}
        for record in records:
            groups[record.complexity_level].append(record)
        
        # ä»æ¯ç»„æŠ½æ ·
        sampled_records = []
        sample_sizes = {
            'simple': sampling_config['simple_sample_size'],
            'medium': sampling_config['medium_sample_size'],
            'complex': sampling_config['complex_sample_size']
        }
        
        for complexity, group_records in groups.items():
            sample_size = min(sample_sizes[complexity], len(group_records))
            if sample_size > 0:
                sampled = random.sample(group_records, sample_size)
                sampled_records.extend(sampled)
        
        return sampled_records
    
    def get_targeted_records(self, records: List[ValidationRecord]) -> List[ValidationRecord]:
        """è·å–ä¸“é¡¹å®¡æ ¸è®°å½•"""
        targeted_config = self.config['validation_modes']['targeted_audit']
        targeted_records = []
        
        for record in records:
            # å¤šä½œè€…æ¡ˆä¾‹
            if (targeted_config['multi_author_cases'] and 
                ';' in record.processed_authors):
                targeted_records.append(record)
                continue
            
            # åŒ…å«æœºæ„ä¿¡æ¯æ¡ˆä¾‹
            if (targeted_config['affiliation_cases'] and 
                len(record.processed_affiliations) > 0):
                targeted_records.append(record)
                continue
            
            # ç–‘ä¼¼é”™è¯¯æ¡ˆä¾‹ï¼ˆå¯å‘å¼è§„åˆ™ï¼‰
            if targeted_config['error_prone_cases']:
                # æ£€æŸ¥æ˜æ˜¾çš„é”™è¯¯æ¨¡å¼
                if ('*' in record.processed_authors or
                    record.processed_authors.strip() == '' or
                    len(record.processed_authors.split(';')) != record.author_count):
                    targeted_records.append(record)
        
        return targeted_records
    
    def load_validation_progress(self) -> Dict[str, ValidationRecord]:
        """åŠ è½½éªŒè¯è¿›åº¦"""
        progress_file = Path(self.data_paths['validation_progress'])
        
        if not progress_file.exists():
            return {}
        
        try:
            with open(progress_file, 'r', encoding='utf-8') as f:
                progress_data = json.load(f)
            
            # è½¬æ¢ä¸ºValidationRecordå¯¹è±¡
            progress = {}
            for record_id, data in progress_data.items():
                # å¤„ç†datetimeå­—æ®µ
                if data.get('validation_timestamp'):
                    data['validation_timestamp'] = data['validation_timestamp']
                
                # å¤„ç†åˆ—è¡¨å­—æ®µ
                if not isinstance(data.get('processed_affiliations'), list):
                    data['processed_affiliations'] = []
                
                progress[record_id] = ValidationRecord(**data)
            
            return progress
            
        except Exception as e:
            print(f"âš ï¸ åŠ è½½è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")
            return {}
    
    def save_validation_progress(self, progress: Dict[str, ValidationRecord]) -> None:
        """ä¿å­˜éªŒè¯è¿›åº¦ï¼ˆå¸¦æ•°æ®ä¿æŠ¤ï¼‰"""
        from .data_protection import create_protection_manager
        
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
        serializable_progress = {}
        for record_id, record in progress.items():
            serializable_progress[record_id] = asdict(record)
        
        try:
            # ä½¿ç”¨æ•°æ®ä¿æŠ¤ç®¡ç†å™¨å®‰å…¨ä¿å­˜
            protection = create_protection_manager()
            success = protection.safe_save(serializable_progress)
            
            if success:
                print(f"âœ… éªŒè¯è¿›åº¦å·²å®‰å…¨ä¿å­˜ (è®°å½•æ•°: {len(serializable_progress)})")
            else:
                print(f"âš ï¸ å®‰å…¨ä¿å­˜å¤±è´¥ï¼Œå°è¯•ç›´æ¥ä¿å­˜...")
                # å›é€€åˆ°ç›´æ¥ä¿å­˜
                progress_file = Path(self.data_paths['validation_progress'])
                with open(progress_file, 'w', encoding='utf-8') as f:
                    json.dump(serializable_progress, f, ensure_ascii=False, indent=2)
                print(f"âœ… éªŒè¯è¿›åº¦å·²ç›´æ¥ä¿å­˜ (è®°å½•æ•°: {len(serializable_progress)})")
                
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")
            raise
    
    def save_validation_results(self, records: List[ValidationRecord]) -> None:
        """ä¿å­˜æœ€ç»ˆéªŒè¯ç»“æœåˆ°Parquetæ–‡ä»¶"""
        # è½¬æ¢ä¸ºDataFrame
        data = [asdict(record) for record in records]
        df = pd.DataFrame(data)
        
        # ä¿å­˜ä¸ºParquet
        results_file = Path(self.data_paths['validation_results'])
        pq.write_table(pa.Table.from_pandas(df), results_file)
        
        print(f"âœ… éªŒè¯ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    def get_validation_statistics(self, records: List[ValidationRecord]) -> Dict:
        """è®¡ç®—éªŒè¯ç»Ÿè®¡ä¿¡æ¯"""
        completed_records = [r for r in records if r.overall_status is not None]
        
        if not completed_records:
            return {
                'total_records': len(records),
                'completed_records': 0,
                'completion_rate': 0.0,
                'accuracy_by_complexity': {},
                'overall_accuracy': 0.0
            }
        
        # æŒ‰å¤æ‚åº¦ç»Ÿè®¡å‡†ç¡®ç‡
        accuracy_by_complexity = {}
        for complexity in ['simple', 'medium', 'complex']:
            complexity_records = [r for r in completed_records if r.complexity_level == complexity]
            if complexity_records:
                correct_count = len([r for r in complexity_records if r.overall_status == 'correct'])
                accuracy_by_complexity[complexity] = {
                    'total': len(complexity_records),
                    'correct': correct_count,
                    'accuracy': correct_count / len(complexity_records)
                }
        
        # æ•´ä½“å‡†ç¡®ç‡
        total_correct = len([r for r in completed_records if r.overall_status == 'correct'])
        overall_accuracy = total_correct / len(completed_records)
        
        return {
            'total_records': len(records),
            'completed_records': len(completed_records),
            'completion_rate': len(completed_records) / len(records),
            'accuracy_by_complexity': accuracy_by_complexity,
            'overall_accuracy': overall_accuracy,
            'status_distribution': {
                'correct': len([r for r in completed_records if r.overall_status == 'correct']),
                'incorrect': len([r for r in completed_records if r.overall_status == 'incorrect']),
                'partial': len([r for r in completed_records if r.overall_status == 'partial'])
            }
        }


def main():
    """æµ‹è¯•æ•°æ®ç®¡ç†å™¨åŠŸèƒ½"""
    dm = DataManager()
    
    print("ğŸ”„ åŠ è½½æºæ•°æ®...")
    input_df, output_df = dm.load_source_data()
    print(f"âœ… è¾“å…¥æ•°æ®: {len(input_df)} æ¡è®°å½•")
    print(f"âœ… è¾“å‡ºæ•°æ®: {len(output_df)} æ¡è®°å½•")
    
    print("\nğŸ”„ å‡†å¤‡éªŒè¯è®°å½•...")
    records = dm.prepare_validation_records()
    print(f"âœ… å‡†å¤‡å®Œæˆ: {len(records)} æ¡è®°å½•")
    
    # æ˜¾ç¤ºå¤æ‚åº¦åˆ†å¸ƒ
    complexity_dist = {}
    for record in records:
        complexity_dist[record.complexity_level] = complexity_dist.get(record.complexity_level, 0) + 1
    
    print(f"\nğŸ“Š å¤æ‚åº¦åˆ†å¸ƒ:")
    for complexity, count in complexity_dist.items():
        print(f"  {complexity}: {count} æ¡")
    
    print("\nğŸ”„ åˆ†å±‚æŠ½æ ·æµ‹è¯•...")
    sampled = dm.stratified_sampling(records)
    print(f"âœ… æŠ½æ ·ç»“æœ: {len(sampled)} æ¡è®°å½•")
    
    print("\nğŸ”„ ä¸“é¡¹å®¡æ ¸æµ‹è¯•...")
    targeted = dm.get_targeted_records(records)
    print(f"âœ… ä¸“é¡¹å®¡æ ¸: {len(targeted)} æ¡è®°å½•")


if __name__ == "__main__":
    main()
